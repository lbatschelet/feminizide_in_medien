---
title: "From “Family Tragedy” to “Feminicide”?"
subtitle: "Exploring Discursive Change in Swiss Media Reporting on the Killing of Women since 1990"
abstract: |
  The language used in media reporting on the killing of women reflects and shapes broader social understandings of gender-based violence. This paper presents an exploratory analysis of discursive change in German-language Swiss media reporting on feminicides since 1990. Drawing on the Swissdox media archive, we apply diachronic word2vec models to examine how key terms and their semantic neighbourhoods have evolved over time. Rather than offering definitive causal explanations, the study aims to detect patterns of continuity and change in terminology, framing, and contextual associations. The findings indicate a gradual shift from individualising and relational framings toward more politicised and gender-specific vocabularies, while certain narrative conventions remain remarkably stable. By combining computational text analysis with a critical perspective on discourse, the paper highlights both the potential and the limits of word embedding methods for studying long-term transformations in media representations of feminicide.
date: today
author:
  - name: Lukas Batschelet
    email: lukas.batschelet@students.unibe.ch
  - name: Linus Kiener
    email: linus.kiener@students.unibe.ch
license: "CC-BY-NC-SA"
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    code-fold: true
    code-tools: true
    df-print: paged
    reference-location: section
    code-links: 
      - text: GitHub Repository
        icon: github
        href: https://github.com/lbatschelet/feminizide_in_medien
      - text: Quarto Source
        icon: file-code
        href: Notebook.qmd
      - text: Swissdox API Wrapper
        icon: file-code
        href: swissdox_api.R
execute:
  cache: true
bibliography: library.bib
---

# Introduction

Media coverage of the topic of femicide has always been controversial. The term “femicide” itself is the subject of complex debate in the media and politics. For example, in its statement of August 12, 2020, on interpellation 20.3505[^1], the Federal Council said: “There are currently no plans to use the term ‘femicide’,” in response to the question of whether “\[it\] intends to promote the use of the term ‘femicide’ in diplomacy and in relations with the media”.[^2] However, it also points out that “\[t\]he FSO, on the other hand, \[reports\] the gender of the victims and the relationship between the victims and the accused in the police crime statistics (PKS) for violent crimes, so that homicides against women are visible in the statistics”.[^3] Information such as this can then be used by projects like [stopfemizid.ch](https://stopfemizid.ch) to crowdsource reports of such incidents. However, the debate surrounding the use of different terms has now progressed further, with the term “feminicide” gradually becoming established. (As we can also show in our subsequent analysis.) The term was coined by Mexican feminist activist Marcela Lagarde, who wanted to use the neologism “feminicidio” expanding the concept of “femicidio” to include, among other things, the dimension of institutional violence and responsibility [cf. @saccomano_femicide_2015, pp. 5; as seen in @ggf_femizid_2023]. However, @saccomano_femicide_2015 <!-- [p. 33] --> notes that the popularization of the term has also led to confusion and that it is often mistakenly used for any killing of a woman.

[^1]: Swiss Federal Council, August 12, 2020, https://www.parlament.ch/de/ratsbetrieb/suche-curia-vista/geschaeft?AffairId=20203505 [via @carobbio_giscetti_interpellation_2020] <!-- evtl. heikel in Fussnote zu zitieren? -->

[^2]: German: „\[er\] die Verwendung des Terminus ‘Femizid’ in der Diplomatie und im Umgang mit den Medien zu fördern \[beabsichtigt\]” <!-- evtl. italic? -->

[^3]: German: “\[d\]as BFS hingegen in der polizeilichen Kriminalstatistik (PKS) bei Gewaltstraftaten das Geschlecht der geschädigten Personen sowie die Beziehung zwischen geschädigten und beschuldigten Personen \[ausweist\], so dass Tötungsdelikte an Frauen in der Statistik sichtbar werden” <!-- evtl. italic? -->

In its press release on case No. 20/2025, the Swiss Press Council also stated that “\[...\] it is in the public interest to report on femicides and draw attention to the issue” [@presserat_berichterstattung_2025]. Similar trends can also be observed in academic literature. An overview by @meltzer_gewalt_2023 summarizes extensive findings on media coverage in Germany and mentions “that the term ‘femicide’ did not play a major role in the media until 2019.” According to the report, it was only used in about 1% <!-- escape für % sollte nicht nötig sein --> of the articles examined. We will attempt to replicate this finding, among others, for Switzerland using data from Swissdox via the LiRI (Linguistic Research Infrastructure) API. Moreover, @wetzstein_femicide_2024 finds that, for posts from the entire DACH region during the observation period between 2018 and 2020, euphemistic terms such as “relationship drama” or “blood deed” vary significantly depending on the type of relationship between the victim and the perpetrator. These and other terms such as “family tragedies” or “jealousy dramas” “trivialize violence against women and turn femicides into fateful isolated cases” [@journalistde_kein_2023]. Apart from that, the use of terms such as “femicide” (which focus on the gender of the victim) is more common when the perpetrator's migrant background is mentioned in parallel [@wetzstein_femicide_2024; via @ustpat_studie_2024].

Based on this initial literature review, we have concluded that there is a substantial need for the availability and analysis of data related to the topic of femi(ni)cide, particularly in the recent past within the Swiss media landscape. Our work aims to construct a framework for obtaining relevant data and then demonstrates possible approaches and methods of analysis.

This paper asks whether Swiss media language has shifted from euphemistic or relational framings (e.g., “family tragedy”) toward more explicit, feminist terms (e.g., “femicide”). We combine a two-stage retrieval from Swissdox with descriptive frequency analysis and diachronic word embeddings. The aim is not causal explanation but a systematic description of framing change over time.

<!-- *** -->

<!-- redundant durch fussnoten separator -->

<!-- Achtung: Quarto sollte gehen, aber R Markdown ist "lightweight markup" und unterstützt fussnoten nicht -->

# First iteration

The first iteration is exploratory: we do not assume that we already know how feminicide was framed in earlier decades. We therefore start with a broad query and use word2vec to identify terms that appear in similar contexts to our initial seed list. This step helps surface historically plausible expressions that may not be captured by contemporary labels. The goal is not to finalize a vocabulary yet, but to map the semantic neighborhood around feminicide-related discourse. The resulting candidate terms become the basis for refining the second query. In short, iteration one is a discovery phase that expands the vocabulary and reduces present-day bias.

## Setup

This section initializes the environment, dependencies, and shared configuration used across all analyses. We load the packages that define the analytical toolbox (data handling, text processing, and visualization) so that later steps are reproducible. The emphasis is on making the computational environment explicit, because results in text analysis can depend on package versions and tokenization defaults. We also flag the computational cost early to set expectations about runtime. This is important because several blocks are intentionally heavy and should not be re-run accidentally. The setup therefore establishes a clear boundary between “infrastructure” and “analysis,” so later sections can focus on interpretation.

::: {.callout-warning}
### Large files and long compute times ahead!

This pipeline relies on multithreaded processing wherever possible.

Please make sure that **`quanteda` is running in parallel mode**.\
If parallelisation is disabled, several steps (especially tokenisation and feature extraction) may take *extremely* long.

::: {.callout-tip collapse=true}
### How to check whether `quanteda` runs in parallel

Because tokenization and DFM construction are the dominant cost drivers, parallelization has a large impact on total runtime. If multithreading is disabled, later blocks may take hours or fail due to time limits. We therefore make this check explicit and easy to repeat. It also increases reproducibility by ensuring that performance differences are not mistaken for methodological differences. In short, this is a practical prerequisite for running the pipeline reliably.

In R, run (quanteda is already loaded in the setup chunk):

``` r
quanteda_options()
```

If you see:

```
Parallel computing: disabled
```

then multithreading is **not** active and performance will be severely reduced.

You can also inspect:

``` r
quanteda_options()
```

For setup instructions and troubleshooting, see: <https://quanteda.io/articles/pkgdown/parallelisation.html>
:::

Swissdox API calls are also inherently slow and may take hours for large queries.
:::

```{r setup, cache=FALSE}
required_packages <- c(
  "httr", "jsonlite", "yaml", "dotenv", "R6",
  "dplyr", "ggplot2", "stringr", "tidytext", "readr", "tidyr",
  "quanteda", "quanteda.textplots", "quanteda.textstats",
  "tm", "topicmodels", "wordcloud", "RColorBrewer",
  "umap", "plotly", "hunspell", "word2vec", "progress",
  "qs2", "tibble", "scales"
)

missing_packages <- required_packages[
  !vapply(required_packages, requireNamespace, logical(1), quietly = TRUE)
]
if (length(missing_packages) > 0) {
  install.packages(missing_packages)
}

invisible(lapply(required_packages, function(pkg) {
  suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}))

# API Wrapper laden
source("swissdox_api.R")

```

## Configuration

Centralized API access, checkpoint utilities, and text configuration. We define API credentials and guard flags here to prevent accidental re-fetching of large datasets. Checkpoint helpers standardize saving and loading of intermediate objects, which keeps the pipeline restartable and transparent. The same block also defines text-processing defaults (e.g., stemming and compound patterns) so that all later analyses rely on the same linguistic assumptions. This is crucial for comparability across time windows: we want observed shifts to reflect discourse change, not shifting preprocessing settings. By keeping configuration centralized, we can document methodological choices once and minimize hidden changes.

```{r document-setup}
# Load environment variables from local.env
if (file.exists("local.env")) {
  load_dot_env("local.env")
  message("\u2713 Environment variables loaded from local.env")
} else {
  message("WARN: local.env not found - using system environment")
}

# Initialize API client with credentials
client <- SwissdoxAPI$new(
  api_key = Sys.getenv("SWISSDOX_API_KEY"),
  api_secret = Sys.getenv("SWISSDOX_API_SECRET")
)

# Guard flags to prevent accidental duplicate API calls
# Query guards:
# - RUN_* enables the API call at all
# - FORCE_* re-runs even if a checkpoint already exists
RUN_FIRST_PASS_QUERY <- FALSE
FORCE_FIRST_PASS_QUERY <- FALSE
RUN_SECOND_PASS_QUERY <- FALSE
FORCE_SECOND_PASS_QUERY <- FALSE
# Word2Vec guards (first-pass)
RUN_WORD2VEC_TRAINING <- TRUE
FORCE_WORD2VEC_TRAINING <- FALSE
# Second-pass Word2Vec guards
RUN_SECOND_PASS_W2V_TRAINING <- TRUE
FORCE_SECOND_PASS_W2V_TRAINING <- FALSE
# Checkpoint helpers (load/save latest timestamped files)
find_latest <- function(pattern, dir = "data") {
  files <- list.files(dir, pattern = pattern, full.names = TRUE)
  if (length(files) == 0) return(NULL)
  # Prefer timestamp in filename (YYYYMMDD_HHMMSS). Fall back to mtime.
  stamps <- stringr::str_match(basename(files), "(\\d{8}_\\d{6})")[, 2]
  if (all(is.na(stamps))) {
    return(files[order(file.mtime(files), decreasing = TRUE)][1])
  }
  ts <- suppressWarnings(as.POSIXct(stamps, format = "%Y%m%d_%H%M%S", tz = "UTC"))
  files[order(ts, file.mtime(files), decreasing = TRUE)][1]
}

latest_checkpoint <- function(prefix, dir = "data") {
  patterns <- c(
    paste0("^", prefix, "_\\d{8}_\\d{6}\\.rds$"),
    paste0("^", prefix, "_\\d{8}_\\d{6}\\.qs2$")
  )
  files <- unlist(lapply(patterns, function(p) list.files(dir, pattern = p, full.names = TRUE)))
  if (length(files) == 0) return(NULL)
  # Reuse find_latest logic on the combined list
  stamps <- stringr::str_match(basename(files), "(\\d{8}_\\d{6})")[, 2]
  if (all(is.na(stamps))) {
    return(files[order(file.mtime(files), decreasing = TRUE)][1])
  }
  ts <- suppressWarnings(as.POSIXct(stamps, format = "%Y%m%d_%H%M%S", tz = "UTC"))
  files[order(ts, file.mtime(files), decreasing = TRUE)][1]
}

save_checkpoint <- function(obj, prefix, dir = "data", format = "qs2", compress = "xz") {
  timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
  if (format == "qs2") {
    if (!requireNamespace("qs2", quietly = TRUE)) {
      stop("qs2 package not installed, cannot save .qs2 checkpoint")
    }
    file <- file.path(dir, paste0(prefix, "_", timestamp, ".qs2"))
    qs2::qs_save(obj, file)
  } else {
    file <- file.path(dir, paste0(prefix, "_", timestamp, ".rds"))
    saveRDS(obj, file, compress = compress)
  }
  message("Saved checkpoint: ", basename(file))
  file
}

load_checkpoint <- function(prefix, var_name) {
  file <- latest_checkpoint(prefix)
  if (is.null(file)) {
    message("No checkpoint found for: ", prefix)
    return(NULL)
  }
  if (grepl("\\.qs2$", file)) {
    if (!requireNamespace("qs2", quietly = TRUE)) {
      stop("qs2 package not installed, cannot read: ", basename(file))
    }
    assign(var_name, qs2::qs_read(file), envir = .GlobalEnv)
  } else {
    assign(var_name, readRDS(file), envir = .GlobalEnv)
  }
  message("Loaded checkpoint: ", basename(file))
  file
}
# Text analysis defaults
SAVE_CORPUS <- FALSE
USE_STEMMING <- TRUE
TRAIN_FULL_MODEL <- TRUE
FULL_MODEL_LABEL <- "all_2000-2025"

# Multi-word patterns after stemming (used in preprocessing + word2vec)
simplified_patterns <- list(
  c("hauslich", "gewalt"),
  c("partnerschaft", "gewalt"),
  c("partner", "gewalt"),
  c("bezieh", "gewalt"),
  c("sexuell", "gewalt"),
  c("körperl", "gewalt"),
  c("psychisch", "gewalt"),
  c("töt", "frau"),
  c("mord", "frau"),
  c("gewalt", "frau"),
  c("famili", "drama"),
  c("famili", "tragödi"),
  c("bezieh", "tat"),
  c("eh", "streit"),
  c("opf", "schutz"),
  c("gewalt", "schutz")
)

# Normalize terms to match Word2Vec vocabulary (lowercase, optional stemming)
normalize_terms_for_model <- function(terms) {
  norm <- stringr::str_to_lower(terms)
  toks <- quanteda::tokens(norm, remove_punct = TRUE)
  if (USE_STEMMING) {
    toks <- quanteda::tokens_wordstem(toks, language = "de")
  }
  vapply(toks, function(x) paste(x, collapse = "_"), character(1))
}

```

## Keyword definitions

These terms are the basis for the queries. We define them centrally to keep usage consistent across analysis and API calls. The list deliberately combines explicit feminicide terms with euphemistic or relational framings to capture competing discourse styles. This allows us to quantify whether explicit terms rise while euphemistic terms decline. By declaring the list here, we also make the operationalization transparent and open to critique. Later analyses use the same vocabulary so that shifts are interpretable as changes in usage rather than changes in definitions. This is a core methodological choice for the paper.

```{r keyword-definitions}

feminizid_keywords <- c(
  "Feminizid", "Femizid", "Frauenmord",
  "Familiendrama", "Familientragödie", "Beziehungstat", "Ehestreit",
  "häusliche Gewalt", "Partnerschaftsgewalt", "Gewalt gegen Frauen",
  "Tötung von Frau", "Mord an Frau", "Partnergewalt", "Beziehungsgewalt"
)

```

## Data gathering

We retrieve relevant Swissdox articles with an initial broad keyword query. The goal is to maximize recall so that early analyses are not biased by overly restrictive filters. The API call is guarded by flags and checkpoints, so re-runs can reuse cached results without repeating the full download. This is important because the Swissdox API is slow and rate-limited, and large queries may take hours. We treat this first pass as an exploratory corpus that defines the overall landscape of discourse. It also provides the basis for later refinement into a second, more focused pass.

Build and run the Swissdox query. Use the checkpoint load block to resume without re-running the API call.

We start with a broad exploratory query using the original keyword list (OR).

```{r checkpoint-load-raw, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
# Checkpoint load (raw data)
load_checkpoint("feminizid_raw_data", "media_data")
```

```{r feminizid-data-collection, cache=FALSE}
# Skip if a checkpoint exists unless explicitly forced
existing_firstpass <- latest_checkpoint("feminizid_raw_data")
if (!RUN_FIRST_PASS_QUERY || (!FORCE_FIRST_PASS_QUERY && !is.null(existing_firstpass))) {
  message("First-pass query skipped. Existing checkpoint: ", basename(existing_firstpass))
} else {
# Full data collection with API wrapper
collection_result <- execute_swissdox_workflow(
  client = client,
  query = create_swissdox_query(
    content_terms = feminizid_keywords,
    date_from = "2000-01-01",
    date_to = format(Sys.Date(), "%Y-%m-%d"),
    languages = "de",
    max_results = 1000000L,
    columns = c("id", "pubtime", "medium_code", "medium_name", "rubric",
               "regional", "doctype", "doctype_description", "language", 
               "char_count", "dateline", "head", "subhead", 
               "article_link", "content_id", "content")
  ),
  name = paste("Feminizid_Collection", format(Sys.time(), "%Y%m%d_%H%M%S")),
  comment = "Comprehensive feminicide media analysis dataset",
  return_data = TRUE,
  output_dir = "data"
)

# Store results and save checkpoint
if (!is.null(collection_result$data)) {
  media_data <- collection_result$data
  save_checkpoint(media_data, "feminizid_raw_data")
  message("\u2713 ", collection_result$message)
} else {
  message("Error while loading data")
}
}
```

## Cleaning and preparation

We load raw documents, clean fields, and create time variables for later aggregation. The key steps are de-duplication, removal of empty content, and conversion of timestamps into consistent yearly and monthly variables. This is necessary because publication metadata varies across outlets and years. We also filter to German-language articles to keep the analysis linguistically coherent. All transformations are documented here to keep the analysis pipeline transparent. The result is a standardized dataset that can be used for both descriptive statistics and downstream embedding models.

Load raw data (from checkpoint or TSV), clean fields, and prepare time variables used in analysis.

```{r data-preparation}
# Load data (checkpoint first, TSV fallback)
raw_file <- load_checkpoint("feminizid_raw_data", "media_data")
if (is.null(raw_file)) {
  tsv_files <- list.files("data", pattern = "\\.tsv(\\.xz)?$", full.names = TRUE)
  if (length(tsv_files) == 0) {
    message("WARN: No data files found - run the API call")
  } else {
    message("No .rds file found, loading .tsv files...")
    media_data <- bind_rows(lapply(tsv_files, function(file) {
      message("Loading: ", basename(file))
      read_tsv(file, locale = locale(encoding = "UTF-8"))
    })) %>% distinct(id, .keep_all = TRUE)
    message("\u2713 Combined ", length(tsv_files), " files: ", nrow(media_data), " articles")
    save_checkpoint(media_data, "feminizid_raw_data")
  }
}
```

```{r data-cleaning}

# Prepare data (if not already done)
# Convert dates and create time variables
media_data$pubtime <- as.POSIXct(media_data$pubtime)
media_data$pub_date <- as.Date(media_data$pubtime)
media_data$year <- format(media_data$pub_date, "%Y")
media_data$month <- format(media_data$pub_date, "%m")
media_data$month_year <- format(media_data$pub_date, "%Y-%m")

# Keep German-only articles (drop French)
media_data <- media_data %>%
  filter(language == "de") %>%
  filter(!is.na(content), content != "", nchar(content) > 50)

message("German articles after filtering: ", nrow(media_data))
message("Date range: ", min(media_data$pub_date, na.rm = TRUE), " to ", max(media_data$pub_date, na.rm = TRUE))
message("Media outlets: ", length(unique(media_data$medium_name)))

```

## Intermediary results

We first summarize coverage over time and identify the most frequent terms and sources. These descriptive statistics provide a baseline for interpreting later embedding results.

At this stage we want to know whether the dataset behaves plausibly: are there enough documents per year, and do the most prominent outlets match expectations for Swiss media? These checks are not merely descriptive; they guard against systematic biases introduced by API filters or source coverage. We also look at headline keywords and their temporal patterns to establish whether the vocabulary shift we hypothesize is visible in simple counts. This helps anchor later embedding interpretations in observable frequency trends. In short, the intermediary results provide a sanity check and an empirical baseline.

```{r overview-analysis}

# 1. Articles per year
yearly_counts <- media_data %>%
  count(year) %>%
  filter(!is.na(year), year >= 2000, year <= 2025)

p1 <- ggplot(yearly_counts, aes(x = as.numeric(year), y = n)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Feminicide coverage over time", 
       subtitle = "German media 2000-2025",
       x = "Year", y = "Article count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 2. Top outlets
top_media <- media_data %>%
  count(medium_name, sort = TRUE) %>%
  head(15)

p2 <- ggplot(top_media, aes(x = reorder(medium_name, n), y = n)) +
  geom_col(fill = "coral", alpha = 0.7) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 15 outlets", x = "Outlet", y = "Article count")

# 3. Monthly trend (last 5 years)
recent_trend <- media_data %>%
  filter(year >= 2020) %>%
  count(month_year) %>%
  arrange(month_year)

p3 <- ggplot(recent_trend, aes(x = month_year, y = n)) +
  geom_line(group = 1, color = "darkgreen", size = 1) +
  geom_point(color = "darkgreen", size = 1.5) +
  theme_minimal() +
  labs(title = "Monthly trend 2020-2026", x = "Month", y = "Article count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))

print(p1)
print(p2) 
print(p3)

# Summary
data_overview <- tibble::tibble(
  german_articles = nrow(media_data),
  date_start = min(media_data$pub_date, na.rm = TRUE),
  date_end = max(media_data$pub_date, na.rm = TRUE),
  media_outlets = length(unique(media_data$medium_name)),
  avg_per_year = round(nrow(media_data) / length(unique(media_data$year)))
)
data_overview
```

```{r keyword-analysis}

# Keyword frequency in headlines (central definition)
keyword_counts <- data.frame(keyword = character(), count = numeric())

for (kw in feminizid_keywords) {
  count <- sum(str_detect(media_data$head, regex(kw, ignore_case = TRUE)), na.rm = TRUE)
  keyword_counts <- rbind(keyword_counts, data.frame(keyword = kw, count = count))
}

keyword_counts <- keyword_counts %>%
  filter(count > 0) %>%
  arrange(desc(count))

p4 <- ggplot(keyword_counts, aes(x = reorder(keyword, count), y = count)) +
  geom_col(fill = "darkred", alpha = 0.7) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Keyword frequency in headlines", 
       x = "Keyword", y = "Article count")

print(p4)

# Print top keywords
keyword_counts
```

```{r time-trends}

# Keyword time trends (central definition)
keyword_timeline <- data.frame()

for (kw in feminizid_keywords) {
  yearly_kw <- media_data %>%
    filter(!is.na(year), !is.na(head)) %>%
    mutate(has_keyword = str_detect(head, regex(kw, ignore_case = TRUE))) %>%
    group_by(year) %>%
    summarise(
      total_articles = n(),
      keyword_articles = sum(has_keyword, na.rm = TRUE),
      percentage = (keyword_articles / total_articles) * 100,
      .groups = 'drop'
    ) %>%
    mutate(keyword = kw)
  
  keyword_timeline <- bind_rows(keyword_timeline, yearly_kw)
}

# Keep relevant keywords and years
keyword_timeline <- keyword_timeline %>%
  filter(year >= 2000, year <= 2025) %>%
  group_by(keyword) %>%
  filter(sum(keyword_articles) > 10) %>%  # At least 10 articles overall
  ungroup()

# Absolute counts over time
p5 <- ggplot(keyword_timeline, aes(x = as.numeric(year), y = keyword_articles, color = keyword)) +
  geom_line(size = 1, alpha = 0.8) +
  geom_point(size = 1.5, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Time trends: keyword usage in headlines",
       subtitle = "Absolute article counts per year",
       x = "Year", y = "Article count", color = "Keyword") +
  theme(legend.position = "bottom", legend.text = element_text(size = 8)) +
  guides(color = guide_legend(ncol = 2))

# Heatmap: keywords x years
keyword_matrix <- keyword_timeline %>%
  select(year, keyword, keyword_articles) %>%
  pivot_wider(names_from = keyword, values_from = keyword_articles, values_fill = 0) %>%
  pivot_longer(-year, names_to = "keyword", values_to = "count")

p6 <- ggplot(keyword_matrix, aes(x = as.numeric(year), y = keyword, fill = count)) +
  geom_tile(color = "white", size = 0.1) +
  scale_fill_gradient(low = "white", high = "darkred", name = "Articles") +
  theme_minimal() +
  labs(title = "Keyword intensity over time", x = "Year", y = "Keyword") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(size = 9))

print(p5)
print(p6)

# Key trends
trend_summary <- keyword_timeline %>%
  group_by(keyword) %>%
  summarise(
    total_articles = sum(keyword_articles),
    peak_year = year[which.max(keyword_articles)],
    peak_count = max(keyword_articles),
    .groups = 'drop'
  ) %>%
  arrange(desc(total_articles))

trend_summary
```

```{r target-term-trends}
# Normalized frequency over time for grouped target terms
USE_SMOOTH <- TRUE

term_groups <- list(
  euphemistic = c("Familiendrama", "Familientragödie", "Beziehungstat", "Ehestreit"),
  explicit = c("Femizid", "Feminizid", "Frauenmord"),
  violence_perpetrator = c("häusliche Gewalt", "Misshandlung", "Täter", "Peiniger", "Opfer", "Eifersucht")
)

target_terms_focus <- unique(unlist(term_groups))

media_terms <- media_data %>%
  filter(!is.na(content), content != "") %>%
  mutate(
    year = as.numeric(year),
    content_lower = stringr::str_to_lower(content),
    word_count = stringr::str_count(content_lower, "\\S+")
  ) %>%
  filter(!is.na(year), year >= 2000, year <= 2025)

total_words_year <- media_terms %>%
  group_by(year) %>%
  summarise(total_words = sum(word_count), .groups = "drop")

make_term_regex <- function(term) {
  term <- stringr::str_to_lower(term)
  parts <- strsplit(term, "\\s+")[[1]]
  paste0("\\b", paste(parts, collapse = "\\s+"), "\\b")
}

term_counts <- dplyr::bind_rows(lapply(target_terms_focus, function(term) {
  pattern <- make_term_regex(term)
  media_terms %>%
    mutate(count = stringr::str_count(content_lower, pattern)) %>%
    group_by(year) %>%
    summarise(count = sum(count), .groups = "drop") %>%
    mutate(term = term)
}))

group_map <- dplyr::bind_rows(lapply(names(term_groups), function(g) {
  tibble::tibble(term = term_groups[[g]], group = g)
}))

group_counts <- term_counts %>%
  left_join(group_map, by = "term") %>%
  filter(!is.na(group)) %>%
  group_by(year, group) %>%
  summarise(count = sum(count), .groups = "drop") %>%
  left_join(total_words_year, by = "year") %>%
  mutate(freq_per_million = (count / total_words) * 1e6)

p_terms_time <- ggplot(group_counts, aes(x = year, y = freq_per_million, color = group)) +
  geom_line(alpha = 0.9, linewidth = 1) +
  theme_minimal() +
  labs(
    title = "Framing groups over time (per million words)",
    subtitle = "Euphemistic vs explicit vs violence/perpetrator framing",
    x = "Year", y = "Frequency per million words", color = "Group"
  ) +
  theme(legend.position = "bottom", legend.text = element_text(size = 9))

if (USE_SMOOTH) {
  p_terms_time <- p_terms_time + geom_smooth(se = FALSE, linewidth = 0.8, alpha = 0.4)
}

print(p_terms_time)
```

## Model preparation (corpus and DFM)

Build a quanteda corpus and a trimmed DFM from the cleaned articles. Use the checkpoint load block to resume without reprocessing. We create a linguistically consistent representation of the texts by removing boilerplate, normalizing case, and applying tailored stopword lists. This reduces noise from high-frequency function words and newsroom metadata. The document-feature matrix is the standard input for frequency analysis and embedding pipelines, so this step is foundational. Trimming parameters remove extremely rare and overly frequent terms to balance coverage with computational feasibility. These choices are methodological and therefore documented explicitly.

```{r checkpoint-load-corpus, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
# Checkpoint load (corpus/dfm)
load_checkpoint("feminizid_corpus", "feminizid_corpus")
load_checkpoint("feminizid_dfm", "feminizid_dfm_trimmed")
```

```{r corpus-setup}

corpus_data <- media_data %>%
  distinct(id, .keep_all = TRUE) %>%
  filter(!is.na(content), content != "", nchar(content) > 100) %>%
  mutate(unique_id = paste0("doc_", row_number()))

message("Articles for corpus: ", nrow(corpus_data))

corpus_data$clean_content <- corpus_data$content %>%
  str_remove_all("<[^>]+>") %>%
  str_remove_all("\\n|\\t|\\r") %>%
  str_remove_all("https?://\\S+") %>%
  str_squish()

corpus_data <- corpus_data %>%
  filter(!is.na(clean_content), clean_content != "", nchar(clean_content) > 50)

feminizid_corpus <- corpus(corpus_data,
                            text_field = "clean_content",
                            docid_field = "unique_id")

docvars(feminizid_corpus, "original_id") <- corpus_data$id
docvars(feminizid_corpus, "medium") <- corpus_data$medium_name
docvars(feminizid_corpus, "date") <- corpus_data$pub_date
docvars(feminizid_corpus, "year") <- corpus_data$year
docvars(feminizid_corpus, "headline") <- corpus_data$head
docvars(feminizid_corpus, "char_count") <- as.numeric(corpus_data$char_count)

corpus_summary <- tibble::tibble(
  documents = ndoc(feminizid_corpus),
  tokens = sum(ntoken(feminizid_corpus)),
  date_start = min(corpus_data$pub_date, na.rm = TRUE),
  date_end = max(corpus_data$pub_date, na.rm = TRUE)
)
corpus_summary

summary(feminizid_corpus, n = 3)

save_checkpoint(feminizid_corpus, "feminizid_corpus")
```

```{r text-preprocessing}

# Load German stopwords
german_stopwords <- readLines("german_stopwords_full.txt", encoding = "UTF-8")
german_stopwords <- german_stopwords[german_stopwords != ""]

# Media-specific stopwords
media_stopwords <- c("uhr", "jahr", "jahre", "jahren", "prozent", "franken", "chf",
                    "artikel", "bericht", "zeitung", "medium", "redaktion", "autor",
                    "quelle", "foto", "bild", "video", "online", "www", "http",
                    "montag", "dienstag", "mittwoch", "donnerstag", "freitag",
                    "samstag", "sonntag", "januar", "februar", "märz", "april",
                    "mai", "juni", "juli", "august", "september", "oktober",
                    "november", "dezember", "schweiz", "schweizer", "schweizerin")

all_stopwords <- c(german_stopwords, media_stopwords)
all_stopwords <- unique(stringr::str_to_lower(all_stopwords))
all_stopwords <- all_stopwords[nzchar(all_stopwords)]


# Tokenisierung und Preprocessing
feminizid_tokens <- tokens(feminizid_corpus,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_numbers = TRUE,
                          remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(all_stopwords, min_nchar = 3, valuetype = "fixed", padding = FALSE)

# Stemming (optional)
if (USE_STEMMING) {
  feminizid_tokens <- tokens_wordstem(feminizid_tokens, language = "de")
}

# Then multi-word expressions (now with base forms)
# Simplified patterns - base forms are sufficient
# Uses simplified_patterns defined in text-config

# Compound multi-word expressions into single tokens
for (pattern in simplified_patterns) {
  feminizid_tokens <- tokens_compound(feminizid_tokens, 
                                      pattern = phrase(paste(pattern, collapse = " ")),
                                      concatenator = "_")
}

preprocessing_steps <- tibble::tibble(
  step = c("Stemming", "Multi-word expressions", "Benefit"),
  detail = c(
    "Reduces inflectional variants (e.g., häusliche→häusl, tötung→töt)",
    paste(length(simplified_patterns), "word pairs compounded (e.g., häusl + gewalt → häusl_gewalt)"),
    "One pattern captures multiple variants"
  )
)
preprocessing_steps

# Document-Feature-Matrix
feminizid_dfm <- dfm(feminizid_tokens)

# Trimming: consistent parameters (both as proportions)
# Based on quanteda documentation: https://quanteda.io/reference/dfm_trim.html
feminizid_dfm_trimmed <- dfm_trim(feminizid_dfm, 
                                  min_docfreq = 0.0003,  # Min 0.03% (~11 docs at 35k)
                                  max_docfreq = 0.90,    # Max 90% of documents
                                  docfreq_type = "prop")  # Both as proportions

preprocessing_summary <- tibble::tibble(
  documents = ndoc(feminizid_dfm),
  original_features = nfeat(feminizid_dfm),
  trimmed_features = nfeat(feminizid_dfm_trimmed),
  sparsity = round(sparsity(feminizid_dfm_trimmed), 3)
)
preprocessing_summary

# Most frequent words
if (nfeat(feminizid_dfm_trimmed) > 0) {
  top_features <- topfeatures(feminizid_dfm_trimmed, 25)
  top_features_df <- tibble::tibble(
    term = names(top_features),
    frequency = as.numeric(top_features)
  )
  top_features_df
} else {
  message("WARN: No features after trimming - adjust parameters")
}
```

```{r checkpoint-save-dfm, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
# Checkpoint save (DFM and optional corpus)
if (nfeat(feminizid_dfm_trimmed) > 0) {
  save_checkpoint(feminizid_dfm_trimmed, "feminizid_dfm")
}
if (SAVE_CORPUS) {
  save_checkpoint(feminizid_corpus, "feminizid_corpus")
}
```

```{r visualization}
# Use quanteda.textplots if available, otherwise fallback to wordcloud
if ("textplot_wordcloud" %in% ls("package:quanteda.textplots")) {
  textplot_wordcloud(feminizid_dfm_trimmed, 
                    min_count = 30,
                    max_words = 100,
                    color = brewer.pal(8, "Dark2"))
} else {
  # Alternative mit wordcloud package
  freq_data <- topfeatures(feminizid_dfm_trimmed, 100)
  wordcloud(names(freq_data), freq_data, 
           min.freq = 30, max.words = 100,
           colors = brewer.pal(8, "Dark2"),
           random.order = FALSE)
}

# Feature frequency over time (normalized)
# Top 10 features by year
top_terms <- names(topfeatures(feminizid_dfm_trimmed, 10))
  
# Frequency of these terms over years
yearly_terms <- dfm_group(feminizid_dfm_trimmed, groups = docvars(feminizid_dfm_trimmed, "year"))

# Articles per year for normalization
articles_per_year <- feminizid_corpus %>%
  docvars() %>%
  mutate(year = as.numeric(year)) %>%  # Convert year to numeric
  count(year, name = "total_articles") %>%
  filter(year >= 2000, year <= 2025)

# Correct conversion for quanteda 3.0+ - without doc_id column
term_data <- convert(yearly_terms[, top_terms], to = "data.frame") %>%
  select(-doc_id) %>%  # Remove doc_id column
  mutate(year = as.numeric(rownames(yearly_terms)))

# Absolute frequencies
term_timeline_abs <- term_data %>%
  pivot_longer(-year, names_to = "term", values_to = "frequency") %>%
  filter(year >= 2000, year <= 2025)

# Normalized frequencies (per 1000 articles)
term_timeline_norm <- term_timeline_abs %>%
  left_join(articles_per_year, by = "year") %>%
  mutate(frequency_per_1000 = (frequency / total_articles) * 1000) %>%
  filter(!is.na(total_articles), total_articles > 10)  # Only years with >10 articles

# Plot 1: Absolute frequencies
p7a <- ggplot(term_timeline_abs, aes(x = year, y = frequency, color = term)) +
  geom_line(size = 1, alpha = 0.8) +
  theme_minimal() +
  labs(title = "Most frequent terms over time (absolute)", 
        x = "Year", y = "Absolute frequency", color = "Term") +
  theme(legend.position = "bottom")

# Plot 2: Normalized frequencies (per 1000 articles)
p7b <- ggplot(term_timeline_norm, aes(x = year, y = frequency_per_1000, color = term)) +
  geom_line(size = 1, alpha = 0.8) +
  theme_minimal() +
  labs(title = "Most frequent terms over time (normalized per 1000 articles)", 
        x = "Year", y = "Frequency per 1000 articles", color = "Term") +
  theme(legend.position = "bottom")

# Plot 3: Article distribution over time (sanity check)
p7c <- ggplot(articles_per_year, aes(x = year, y = total_articles)) +
  geom_line(size = 1, color = "darkblue") +
  geom_point(size = 2, color = "darkblue") +
  theme_minimal() +
  labs(title = "Article distribution over time", 
        x = "Year", y = "Article count") +
  scale_y_continuous(labels = scales::comma)

print(p7a)
print(p7b) 
print(p7c)

# Normalization summary
summary_stats <- articles_per_year %>%
  summarise(
    min_articles = min(total_articles),
    max_articles = max(total_articles),
    mean_articles = round(mean(total_articles)),
    median_articles = median(total_articles),
    ratio_max_min = round(max(total_articles) / min(total_articles), 1)
  )
summary_stats

if (summary_stats$ratio_max_min > 5) {
  message("WARN: Strong distortion detected - normalized view recommended")
}

```

## Model creation (Word2Vec, diachronic)

We train separate embeddings for five-year windows and compare nearest neighbors of key terms. This lets us observe shifts in semantic neighborhoods without implying causality.

The diachronic setup is chosen because embeddings are not comparable across time without explicit alignment. By training separate windows, we respect the changing corpus composition and avoid conflating temporal effects. The neighbor comparisons are interpreted as shifts in context rather than stable “meanings.” This is a qualitative signal, not a deterministic metric. We also cache vectors to avoid repeated training, which is computationally expensive. The focus is therefore on reproducible, transparent exploratory analysis rather than performance optimization.

Train one Word2Vec model per time window and compare nearest neighbors across periods. Use the checkpoint load block to resume without retraining.

```{r checkpoint-load-word2vec, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
# Checkpoint load (Word2Vec)
load_checkpoint("word2vec_vectors", "word2vec_vectors")
load_checkpoint("word2vec_frame_shifts", "all_frame_shifts")
```

```{r word2vec-setup}
# Time windows (5-year bins)
time_windows <- list(
  "2000-2004" = c(2000, 2004),
  "2005-2009" = c(2005, 2009), 
  "2010-2014" = c(2010, 2014),
  "2015-2019" = c(2015, 2019),
  "2020-2025" = c(2020, 2025)
)

# Target terms for frame-shift analysis (interpretable terms)
target_words <- c(
  "Frauenmord", "Peiniger", "Familiendrama", "Misshandlung",
  "Eifersucht", "Femizid", "Feminizid", "Familientragödie",
  "Beziehungstat", "Ehestreit", "häusliche Gewalt", "Opfer", "Täter"
)

target_map <- data.frame(
  original = target_words,
  normalized = normalize_terms_for_model(target_words),
  stringsAsFactors = FALSE
)

w2v_setup_summary <- tibble::tibble(
  time_windows = length(time_windows),
  target_words = length(target_words),
  strategy = "one model per window",
  full_model = if (TRAIN_FULL_MODEL) FULL_MODEL_LABEL else NA_character_
)
w2v_setup_summary
```

```{r word2vec-training}

# Train models for each window + optional full-period model
existing_vectors <- latest_checkpoint("word2vec_vectors")
if (!RUN_WORD2VEC_TRAINING) {
  message("Word2Vec training disabled (RUN_WORD2VEC_TRAINING = FALSE).")
  if (!is.null(existing_vectors)) {
    load_checkpoint("word2vec_vectors", "word2vec_vectors")
  }
} else if (!FORCE_WORD2VEC_TRAINING && !is.null(existing_vectors)) {
  message("Word2Vec vectors already exist; skipping training.")
  load_checkpoint("word2vec_vectors", "word2vec_vectors")
} else {
  word2vec_models <- list()

# Optional progress bar (per window)
use_progress <- requireNamespace("progress", quietly = TRUE)
if (!use_progress) {
  message("Tip: install.packages('progress') for a progress bar.")
}

build_w2v_text <- function(corpus_obj) {
  tokens_obj <- tokens(corpus_obj,
                        remove_punct = TRUE,
                        remove_symbols = TRUE,
                        remove_numbers = TRUE,
                        remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(all_stopwords, min_nchar = 3, valuetype = "fixed", padding = FALSE)
  if (USE_STEMMING) {
    tokens_obj <- tokens_wordstem(tokens_obj, language = "de")
  }
  for (pattern in simplified_patterns) {
    tokens_obj <- tokens_compound(tokens_obj,
                                  pattern = phrase(paste(pattern, collapse = " ")),
                                  concatenator = "_")
  }
  txt <- sapply(tokens_obj, paste, collapse = " ")
  txt[nchar(txt) > 50]
}

train_w2v <- function(label, corpus_obj) {
  n_docs <- ndoc(corpus_obj)
  txt <- build_w2v_text(corpus_obj)
  if (n_docs < 100 || length(txt) < 50) {
    message("WARN: Skip ", label, " - too little data")
    return(NULL)
  }
  message("Training Word2Vec for ", label, " (", n_docs, " docs, ", length(txt), " texts)...")
  w2v_args <- list(
    x = txt,
    type = "skip-gram",
    dim = 100,
    iter = 20,
    min_count = 5,
    window = 5,
    threads = max(1, parallel::detectCores() - 1)
  )
  do.call(word2vec::word2vec, w2v_args)
}

pb <- if (use_progress) progress::progress_bar$new(
  format = "Training :current/:total [:bar] :percent :eta",
  total = length(time_windows),
  clear = FALSE,
  width = 60
) else NULL

  for (window_name in names(time_windows)) {
    if (use_progress) pb$tick()
    window_years <- time_windows[[window_name]]
    window_corpus <- corpus_subset(
      feminizid_corpus,
      year >= window_years[1] & year <= window_years[2]
    )
    model <- train_w2v(window_name, window_corpus)
    if (is.null(model)) next
    word2vec_models[[window_name]] <- model
  }

  if (TRAIN_FULL_MODEL) {
    full_model <- train_w2v(FULL_MODEL_LABEL, feminizid_corpus)
    if (!is.null(full_model)) {
      word2vec_models[[FULL_MODEL_LABEL]] <- full_model
    }
  }

  message("Training done. Models: ", length(word2vec_models))

  word2vec_vectors <- lapply(word2vec_models, as.matrix)
  save_checkpoint(word2vec_vectors, "word2vec_vectors")
}
```

```{r word2vec-frame-shift}
if (is.null(word2vec_vectors)) {
  stop("No word2vec vectors available. Run the training chunk once.")
}

window_models <- intersect(names(word2vec_vectors), names(time_windows))
if (length(window_models) >= 2) {
  
  # Frame-shift analysis: what is near what?
  frame_shift_results <- list()

  normalize_vectors <- function(vectors) {
    vectors <- as.matrix(vectors)
    norms <- sqrt(rowSums(vectors^2))
    norms[norms == 0] <- NA_real_
    vectors / norms
  }

  vectors_norm_list <- lapply(word2vec_vectors[window_models], normalize_vectors)
  
  nearest_terms <- function(vectors_norm, term, top_n = 10) {
    if (!term %in% rownames(vectors_norm)) return(NULL)
    sims <- as.numeric(vectors_norm %*% t(vectors_norm[term, , drop = FALSE]))
    names(sims) <- rownames(vectors_norm)
    sims <- sims[names(sims) != term]
    top <- sort(sims, decreasing = TRUE)[seq_len(min(top_n, length(sims)))]
    data.frame(
      neighbor = names(top),
      similarity = as.numeric(top),
      rank = seq_along(top),
      stringsAsFactors = FALSE
    )
  }

  for (i in seq_len(nrow(target_map))) {
    target_word <- target_map$original[i]
    target_norm <- target_map$normalized[i]
    
    neighbors_over_time <- list()
    
    for (window_name in window_models) {
      vectors_norm <- vectors_norm_list[[window_name]]
      nn <- nearest_terms(vectors_norm, target_norm, top_n = 10)
      if (!is.null(nn) && nrow(nn) > 0) {
        neighbors_df <- nn %>%
          mutate(period = window_name, target = target_word) %>%
          select(period, target, neighbor, similarity, rank)
        neighbors_over_time[[window_name]] <- neighbors_df
      }
    }
    
    if (length(neighbors_over_time) > 0) {
      combined_neighbors <- do.call(rbind, neighbors_over_time)
      frame_shift_results[[target_word]] <- combined_neighbors
    }
  }
  
  if (length(frame_shift_results) > 0) {
    all_frame_shifts <- do.call(rbind, frame_shift_results)
    save_checkpoint(all_frame_shifts, "word2vec_frame_shifts")

    frame_shift_summary <- tibble::tibble(
      analyzed_terms = length(frame_shift_results),
      total_neighbor_relations = nrow(all_frame_shifts)
    )
    frame_shift_summary

    top_neighbors_all <- all_frame_shifts %>%
      filter(rank <= 3) %>%
      arrange(target, period, rank)
    top_neighbors_all
  }
}
```

These plots visualize which neighbors appear around each target term in different time windows. The heatmaps highlight continuity and change in contextual proximity, while the stability summary aggregates whether relations are stable or volatile across periods. This is meant as a descriptive lens on shifting discourse, not a definitive semantic map.

```{r word2vec-visualization}

# Visualisierung 1: Heatmap der Frame-Shifts
# Top neighbors across all time windows
top_shifts <- all_frame_shifts %>%
  filter(rank <= 5) %>%
  group_by(target, neighbor) %>%
  summarise(avg_similarity = mean(similarity), 
            periods_present = n(),
            .groups = 'drop') %>%
  filter(periods_present >= 2) %>%  # At least in 2 time windows
  arrange(target, -avg_similarity)

# Heatmap for selected target terms
selected_targets <- c("hauslich_gewalt", "feminizid", "famili_drama")
available_targets <- intersect(selected_targets, unique(all_frame_shifts$target))

if (length(available_targets) > 0) {
  heatmap_data <- all_frame_shifts %>%
    filter(target %in% available_targets, rank <= 5) %>%
    select(period, target, neighbor, similarity)
  
  p_heatmap <- ggplot(heatmap_data, aes(x = period, y = neighbor, fill = similarity)) +
    geom_tile(color = "white") +
    facet_wrap(~target, scales = "free_y") +
    scale_fill_gradient(low = "white", high = "darkred", name = "Ähnlichkeit") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(size = 8)) +
    labs(title = "Word2Vec frame shifts over time",
       subtitle = "Which words are close to the target terms?",
       x = "Time window", y = "Neighbor terms")
  
  print(p_heatmap)
}

# Plot per target (top 5 neighbors per time window)
for (term in unique(all_frame_shifts$target)) {
  term_data <- all_frame_shifts %>%
    filter(target == term, rank <= 5) %>%
    select(period, neighbor, similarity)

  if (nrow(term_data) == 0) next

  p_term <- ggplot(term_data, aes(x = period, y = neighbor, fill = similarity)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "white", high = "darkred", name = "Similarity") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(size = 8)) +
    labs(title = paste("Word2Vec frame shifts:", term),
         x = "Time window", y = "Neighbors")

  print(p_term)
}

# Visualization 2: stability vs. change
stability_analysis <- all_frame_shifts %>%
  group_by(target, neighbor) %>%
  summarise(
    periods_present = n(),
    avg_similarity = mean(similarity),
    similarity_sd = sd(similarity),
    .groups = 'drop'
  ) %>%
  mutate(
      stability = case_when(
        periods_present >= 4 & similarity_sd < 0.1 ~ "Stable",
        periods_present >= 3 & similarity_sd < 0.2 ~ "Moderately stable", 
        TRUE ~ "Changing"
      )
  )

stability_summary <- stability_analysis %>%
  count(stability) %>%
  mutate(percentage = round(n / sum(n) * 100, 1))

stability_summary
```



## Embedding diagnostics

We reduce the 100-dimensional Word2Vec space to 3D with UMAP to inspect local semantic neighborhoods. UMAP builds a nearest-neighbor graph in the original space and optimizes a low-dimensional layout that preserves those local neighborhoods. The plotted axes are not interpretable dimensions; they are coordinates of a layout that preserves local structure.

Interpretation: - Points close in 3D usually appear in similar contexts in the corpus. - Tight clusters suggest a coherent framing or shared discourse context. - Isolated points can indicate rare usage or distinct framing.

Caveats: - Distances are mainly meaningful locally (nearest neighbors). - Global positions and axis directions should not be over-interpreted.

This diagnostic visualization is meant to communicate local structure, not to claim stable global geometry. It helps verify that target terms sit near plausible contextual neighbors and that the model is not dominated by artifacts. We therefore use it as a sanity check and a didactic tool for readers unfamiliar with embeddings. Any apparent “movement” here should be interpreted together with the more formal neighbor tables above. In short, the plot supports interpretation; it does not replace it.

```{r word2vec-local-embedding}
set.seed(123)

if (is.null(word2vec_vectors)) {
  stop("No word2vec vectors available. Run the training chunk once.")
}

# Prefer full-period model if available, otherwise latest window
if (FULL_MODEL_LABEL %in% names(word2vec_vectors)) {
  viz_window <- FULL_MODEL_LABEL
} else {
  available_windows <- intersect(names(word2vec_vectors), names(time_windows))
  if (length(available_windows) > 0) {
    viz_window <- tail(available_windows, 1)
  } else {
    viz_window <- tail(names(word2vec_vectors), 1)
  }
}
message("Using window: ", viz_window)
vectors <- word2vec_vectors[[viz_window]]

normalize_vectors <- function(vectors) {
  norms <- sqrt(rowSums(vectors^2))
  norms[norms == 0] <- NA_real_
  vectors / norms
}

vectors_norm <- normalize_vectors(vectors)
target_terms_model <- unique(target_map$normalized)
in_vocab <- intersect(target_terms_model, rownames(vectors_norm))
if (length(in_vocab) == 0) {
  message("No target words found in model vocabulary.")
} else {
  neighbor_list <- lapply(in_vocab, function(term) {
    sims <- as.numeric(vectors_norm %*% t(vectors_norm[term, , drop = FALSE]))
    names(sims) <- rownames(vectors_norm)
    sims <- sims[names(sims) != term]
    nn <- names(sort(sims, decreasing = TRUE))[1:10]
    unique(c(term, nn))
  })
  vocab <- unique(unlist(neighbor_list))

  vectors_sub <- vectors[vocab, , drop = FALSE]
  umap_coords <- umap::umap(vectors_sub, n_components = 3)$layout

  embedding_df <- data.frame(
    term = vocab,
    x = umap_coords[, 1],
    y = umap_coords[, 2],
    z = umap_coords[, 3],
    type = ifelse(vocab %in% in_vocab, "target", "neighbor"),
    stringsAsFactors = FALSE
  )

  p_umap <- plotly::plot_ly(
    embedding_df,
    x = ~x, y = ~y, z = ~z, color = ~type, text = ~term,
    type = "scatter3d", mode = "markers",
    marker = list(size = 7, opacity = 0.85)
  ) %>%
    plotly::layout(
      title = paste("Word2Vec embeddings (UMAP 3D):", viz_window),
      scene = list(
        xaxis = list(title = "UMAP dim 1"),
        yaxis = list(title = "UMAP dim 2"),
        zaxis = list(title = "UMAP dim 3")
      )
    )
  p_umap
}
```

## Iterative query expansion

This section uses embedding neighborhoods to propose additional terms that might capture earlier or alternative framings. The candidates are heuristically filtered to reduce noise before being considered for a second-pass query.

We treat this as a hypothesis-generation step: embeddings suggest related terms, but they still require human judgment and contextual validation. The filters (frequency thresholds, dictionary checks, and co-occurrence constraints) reduce noise and help avoid purely incidental neighbors. This step is important because it links computational exploration with manual curation, which is a typical workflow in discourse analysis. It also makes the second-pass query more precise by grounding it in observed usage rather than intuition alone. The resulting list should therefore be read as a curated shortlist, not a definitive taxonomy.

We use the word2vec neighborhoods to suggest additional search terms that may reflect earlier or alternative framing. It provides a ranked candidate list for curation which can then be added to a subsequent API query in an iterative proggress.

```{r query-expansion}
# Split original keywords into action vs. relation/context (no new terms)
action_terms <- c(
  "Feminizid", "Femizid", "Frauenmord",
  "Tötung von Frau", "Mord an Frau",
  "Gewalt gegen Frauen", "Partnergewalt", "Beziehungsgewalt",
  "häusliche Gewalt", "Partnerschaftsgewalt"
)

relation_terms <- c(
  "Familiendrama", "Familientragödie", "Beziehungstat", "Ehestreit"
)

# Seeds in the model vocabulary (normalized form)
seed_terms <- unique(target_map$normalized)

collect_neighbors_vectors <- function(vectors, seeds, top_n = 30) {
  seeds_in_vocab <- intersect(seeds, rownames(vectors))
  if (length(seeds_in_vocab) == 0) return(tibble())

  norms <- sqrt(rowSums(vectors^2))
  norms[norms == 0] <- NA_real_
  vectors_norm <- vectors / norms

  res <- lapply(seeds_in_vocab, function(seed) {
    sims <- as.numeric(vectors_norm %*% t(vectors_norm[seed, , drop = FALSE]))
    names(sims) <- rownames(vectors_norm)
    sims <- sims[names(sims) != seed]
    top <- sort(sims, decreasing = TRUE)[seq_len(min(top_n, length(sims)))]
    data.frame(
      seed = seed,
      term = names(top),
      similarity = as.numeric(top),
      rank = seq_along(top),
      stringsAsFactors = FALSE
    )
  })
  dplyr::bind_rows(res)
}

if (is.null(word2vec_vectors)) {
  stop("No word2vec vectors available. Run the training chunk once.")
}

neighbors_all <- dplyr::bind_rows(lapply(names(word2vec_vectors), function(w) {
  vectors <- word2vec_vectors[[w]]
  df <- collect_neighbors_vectors(vectors, seed_terms, top_n = 30)
  df$window <- w
  df
}))

candidate_summary <- neighbors_all %>%
  filter(!(term %in% seed_terms)) %>%
  group_by(term) %>%
  summarise(
    mean_similarity = mean(similarity),
    max_similarity = max(similarity),
    seed_hits = n_distinct(seed),
    window_hits = n_distinct(window),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_similarity))

# Heuristic: candidates that are close to multiple seeds and appear across windows
suggested_terms <- candidate_summary %>%
  filter(mean_similarity >= 0.35, seed_hits >= 2, window_hits >= 2) %>%
  pull(term)

# Basic cleanup: keep alpha tokens, remove very short / rare items
term_freq <- textstat_frequency(feminizid_dfm_trimmed, n = Inf) %>%
  select(feature, frequency)

suggested_terms_clean <- stringr::str_to_lower(suggested_terms)
suggested_terms_clean <- suggested_terms_clean[
  stringr::str_detect(suggested_terms_clean, "^[[:alpha:]_]+$")
]
suggested_terms_clean <- unique(suggested_terms_clean)
suggested_terms_clean <- Filter(function(x) nchar(x) >= 4, suggested_terms_clean)

suggested_terms_clean <- term_freq %>%
  filter(feature %in% suggested_terms_clean, frequency >= 5) %>%
  arrange(desc(frequency)) %>%
  pull(feature)

# Map stems back to likely surface forms (approximation)
MAP_DOCS_LIMIT <- 10000
corpus_for_map <- if (ndoc(feminizid_corpus) > MAP_DOCS_LIMIT) {
  corpus_sample(feminizid_corpus, size = MAP_DOCS_LIMIT)
} else {
  feminizid_corpus
}

tokens_raw <- tokens(corpus_for_map,
                     remove_punct = TRUE,
                     remove_symbols = TRUE,
                     remove_numbers = TRUE,
                     remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(all_stopwords, min_nchar = 3, valuetype = "fixed", padding = FALSE)

tokens_stem <- tokens_wordstem(tokens_raw, language = "de")

stem_map <- tibble(
  stem = unlist(tokens_stem),
  form = unlist(tokens_raw)
) %>%
  count(stem, form, sort = TRUE)

suggested_terms_surface <- tibble(stem = suggested_terms_clean) %>%
  left_join(
    stem_map %>%
      group_by(stem) %>%
      slice_max(n, n = 3, with_ties = FALSE) %>%
      summarise(forms = paste(form, collapse = ", "), .groups = "drop"),
    by = "stem"
  )

available_dicts <- hunspell::list_dictionaries()
dict_id <- if ("de_DE_frami" %in% available_dicts) {
  "de_DE_frami"
} else if ("de_DE" %in% available_dicts) {
  "de_DE"
} else {
  NA_character_
}

suggested_terms_surface <- suggested_terms_surface %>%
  mutate(
    primary_form = stringr::word(forms, 1, sep = ", "),
    primary_form = stringr::str_replace_all(primary_form, "_", " "),
    dict_ok = if (is.na(dict_id)) TRUE else hunspell::hunspell_check(primary_form, dict = dict_id)
  )

# Co-occurrence filter: candidate should appear with relation terms
relation_terms_model <- normalize_terms_for_model(relation_terms)
dfm_model <- feminizid_dfm_trimmed
relation_in_docs <- rowSums(as.matrix(
  dfm_select(dfm_model, pattern = relation_terms_model)
)) > 0

cand_dfm <- dfm_select(dfm_model, pattern = suggested_terms_clean)
cand_mat <- as.matrix(cand_dfm) > 0
cand_docs <- colSums(cand_mat)
cooccur_docs <- colSums(cand_mat[relation_in_docs, , drop = FALSE])
cooccur_ratio <- cooccur_docs / pmax(cand_docs, 1)

candidate_scores <- tibble(
  stem = colnames(cand_mat),
  cand_docs = cand_docs,
  cooccur_docs = cooccur_docs,
  cooccur_ratio = cooccur_ratio
)

suggested_terms_surface <- suggested_terms_surface %>%
  left_join(candidate_scores, by = "stem")

# Heuristic cleanup for glued function words / artifacts
JUNK_SUFFIXES <- c("zwar", "pro", "und", "oder", "aber", "doch", "weil", "dass", "nicht")
junk_regex <- paste0("(", paste(JUNK_SUFFIXES, collapse = "|"), ")$")

suggested_terms_surface <- suggested_terms_surface %>%
  mutate(junk_suffix = stringr::str_detect(primary_form, junk_regex))

suggested_terms_final <- suggested_terms_surface %>%
  filter(
    dict_ok,
    nchar(primary_form) >= 4,
    !junk_suffix,
    cooccur_docs >= 5,
    cooccur_ratio >= 0.2
  ) %>%
  distinct(primary_form) %>%
  arrange(primary_form) %>%
  pull(primary_form)

suggested_terms_surface %>%
  arrange(desc(primary_form)) %>%
  head(30)

suggested_terms_final
```

# Second iteration

The second iteration operationalizes the expanded vocabulary from iteration one. We run a new query using the enriched term list and then train word2vec models again on this larger, more targeted dataset. The analysis is grouped into five-year windows to make temporal change visible and comparable. This second pass is designed to reveal patterns of shifting usage and meaning, especially the contrast between euphemistic framings and explicit naming. The two-step design thus combines discovery (iteration one) with structured temporal analysis (iteration two). We interpret changes across windows as shifts in contextual association rather than stable semantic truths.

## Data gathering (second-pass query)

The second pass narrows the query by combining action terms with relation/context terms. This reduces false positives while still capturing relevant framing variants. We treat this pass as a curated subset rather than a full census.

Methodologically, this step operationalizes a trade-off: we accept lower recall in exchange for higher precision. This is appropriate for the second iteration because we want a more focused corpus for semantic modeling. The AND combination ensures that both violence/action language and relational framing are present, which targets discourse patterns of interest. We explicitly document the term lists and exclusions to make the curation transparent. This also enables reproducibility and allows readers to critique or revise the operationalization.

We run a stricter AND query to keep results manageable. The original keywords are split into two groups (action vs. relation) and combined with curated extra terms. Wildcards are included for single-word terms.

```{r second-pass-query, cache.lazy=FALSE}
# Wildcards only for single-word terms from the original list
add_wildcards_single <- function(terms) {
  single <- terms[!grepl("\\s", terms)]
  paste0(single, "*")
}

action_terms <- unique(c(action_terms, add_wildcards_single(action_terms)))
relation_terms <- unique(c(relation_terms, add_wildcards_single(relation_terms)))

# Manual split from suggested_terms_final
SECOND_PASS_EXCLUDE <- c("gewalt")

action_terms_extra <- c(
  "ermordet", "erschossen", "erstochen", "getötet",
  "tötung", "tötungsversuch", "frauentötungen",
  "stranguliert", "kopfschuss", "küchenmesser",
  "mordopfer", "umgebracht", "tötete", "totschlag",
  "misshandlung", "misshandelt", "stalking",
  "eifersucht", "eifersüchtig", "eifersüchtige", "eifersüchtigen",
  "mord"

)

relation_terms_extra <- c(
  "ehepartner", "exmann", "intimpartner", "lebensgefährt",
  "lebensgemeinschaft", "Partner", "schwägerin", "Peiniger", "Frau",
  "ehefrau", "freundin", "beziehung", "geschieden", "mutter", "vater",
  "sohn", "tochter", "schwiegersohn", "ex-partner", "ex-mann", "ex-frau"

)

# Dropped terms (noisy/too generic): aufwies, desinteresseerklärung,
# desinteressenerklärung, erleben, erwachsenenleben, fünfte, häusliche,
# körper, männer, menos, mutmassliche, physische, rechercheplattformen,
# rechercheprojekt, sexuelle, straftatbestand, tatverdächtigen, türkin

action_terms_extra <- setdiff(action_terms_extra, SECOND_PASS_EXCLUDE)
relation_terms_extra <- setdiff(relation_terms_extra, SECOND_PASS_EXCLUDE)

action_terms_second <- unique(c(action_terms, action_terms_extra))
relation_terms_second <- unique(c(relation_terms, relation_terms_extra))

normalize_wildcards <- function(terms) {
  terms <- unique(terms)
  terms <- sub("\\*+$", "", terms)
  terms <- unique(terms)
  c(terms, paste0(terms[!grepl("\\s", terms)], "*"))
}

action_terms_second <- normalize_wildcards(action_terms_second)
relation_terms_second <- normalize_wildcards(relation_terms_second)

message("Second-pass action terms: ", paste(action_terms_second, collapse = ", "))
message("Second-pass relation terms: ", paste(relation_terms_second, collapse = ", "))

existing_secondpass <- latest_checkpoint("feminizid_raw_data_secondpass")
if (!RUN_SECOND_PASS_QUERY || (!FORCE_SECOND_PASS_QUERY && !is.null(existing_secondpass))) {
  message("Second-pass query skipped. Existing checkpoint: ", basename(existing_secondpass))
} else {
  second_pass_result <- execute_swissdox_workflow(
    client = client,
    query = create_swissdox_query(
      content_groups = list(
        list(operator = "OR", terms = action_terms_second),
        list(operator = "OR", terms = relation_terms_second)
      ),
      date_from = "1990-01-01",
      date_to = format(Sys.Date(), "%Y-%m-%d"),
      languages = "de",
      max_results = 1000000L,
      columns = c("id", "pubtime", "medium_code", "medium_name", "rubric",
                  "regional", "doctype", "doctype_description", "language",
                  "char_count", "dateline", "head", "subhead",
                  "article_link", "content_id", "content")
    ),
    name = paste("Feminizid_SecondPass", format(Sys.time(), "%Y%m%d_%H%M%S")),
    comment = "Second-pass query with curated action+relation terms (1990+)",
    return_data = TRUE,
    output_dir = "data"
  )

  # Save second-pass result for reuse
  if (!is.null(second_pass_result$data)) {
    save_checkpoint(second_pass_result$data, "feminizid_raw_data_secondpass")
  }
}
```

## Cleaning and preparation (second-pass)

We clean the second-pass data and build a focused corpus for downstream semantic analysis. The same cleaning logic is applied to keep results comparable with the first pass.

This step mirrors the first iteration to ensure comparability. By applying the same filtering and metadata transformations, we reduce the risk that differences in results are artifacts of preprocessing. We also introduce a rubric-based filter to remove overly broad international categories, which helps focus the analysis on more relevant coverage. The output is a second-pass corpus that is smaller but more targeted, which is important for robust embedding comparisons. Consistency across iterations is a methodological requirement for interpreting change.

Clean the second-pass data and build a corpus for downstream analysis.

```{r second-pass-prep, cache.lazy=FALSE}
load_checkpoint("feminizid_raw_data_secondpass", "media_data_second")

media_data_second$pubtime <- as.POSIXct(as.character(media_data_second$pubtime), tz = "UTC")
media_data_second$pub_date <- as.Date(media_data_second$pubtime)
media_data_second$year <- as.numeric(format(media_data_second$pubtime, "%Y"))

media_data_second <- media_data_second %>%
  filter(language == "de") %>%
  filter(!is.na(content), content != "", nchar(content) > 50) %>%
  filter(year >= 1990) %>%
  distinct(id, .keep_all = TRUE) %>%
  mutate(unique_id = paste0("doc_", row_number()))

media_data_second$clean_content <- media_data_second$content %>%
  str_remove_all("<[^>]+>") %>%
  str_remove_all("\\n|\\t|\\r") %>%
  str_remove_all("https?://\\S+") %>%
  str_squish()

secondpass_corpus <- corpus(
  media_data_second,
  text_field = "clean_content",
  docid_field = "unique_id"
)

save_checkpoint(secondpass_corpus, "feminizid_secondpass_corpus")
```

```{r second-pass-filter-rubrics, cache.lazy=FALSE}
# Filter out broad international rubrics for a more focused second-pass set
# (keeps the raw checkpoint intact; only affects downstream analysis)
media_data_second <- media_data_second %>%
  mutate(rubric_clean = stringr::str_to_lower(str_squish(rubric))) %>%
  filter(!rubric_clean %in% c("ausland", "international")) %>%
  select(-rubric_clean)

secondpass_corpus <- corpus(
  media_data_second,
  text_field = "clean_content",
  docid_field = "unique_id"
)
```

## Intermediary results (second-pass)

We summarize the second-pass corpus to check coverage, dominant outlets, and rubric distribution. This ensures the curated query behaves as intended. These summaries serve two purposes. First, they verify that the curated query retrieves a plausible and stable subset of coverage across time. Second, they help interpret later semantic results by showing whether any single outlet or rubric dominates the corpus. We also compute a period contrast to identify terms that are disproportionately associated with later years, which connects directly to the framing-change hypothesis. In short, this section ensures the second-pass dataset is both methodologically sound and substantively interpretable.

```{r second-pass-analysis, cache.lazy=FALSE}
# This block summarizes the curated second-pass corpus.
# Basic distribution: article counts per year (1990-present)
articles_second_by_year <- media_data_second %>%
  mutate(
    pubtime = as.POSIXct(as.character(pubtime), tz = "UTC"),
    year = as.numeric(format(pubtime, "%Y"))
  ) %>%
  filter(!is.na(year), year >= 1990, year <= as.numeric(format(Sys.Date(), "%Y"))) %>%
  count(year)

articles_second_by_year

# Top sources
top_media_second <- media_data_second %>%
  count(medium_name, sort = TRUE) %>%
  head(15)
top_media_second

# Top rubric categories
top_rubrics_second <- media_data_second %>%
  mutate(rubric = na_if(str_squish(rubric), "")) %>%
  count(rubric, sort = TRUE) %>%
  head(15)
top_rubrics_second

# Normalized contrast: early vs. late periods (per million words)
tokens_raw <- tokens(
  secondpass_corpus,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_url = TRUE
)

dfm_raw <- dfm(tokens_raw, tolower = TRUE)
dfm_raw <- dfm_remove(dfm_raw, all_stopwords, valuetype = "fixed")
dfm_raw <- dfm_remove(
  dfm_raw,
  pattern = c(
    "^uhr",
    "^(montag|dienstag|mittwoch|donnerstag|freitag|samstag|sonntag)$",
    ".*\\d+.*",
    "^[a-zäöüß]{1,2}$"
  ),
  valuetype = "regex",
  case_insensitive = TRUE
)
dfm_raw <- dfm_trim(dfm_raw, min_termfreq = 10)

doc_ids <- docnames(dfm_raw)
year_lookup <- media_data_second$year[match(doc_ids, media_data_second$unique_id)]
period_vec <- ifelse(is.na(year_lookup), "Unknown",
                     ifelse(year_lookup < 2005, "1990-2004", "2005-2026"))
docvars(dfm_raw, "period") <- period_vec

dfm_period <- dfm_group(dfm_raw, groups = docvars(dfm_raw, "period"))
dfm_prop <- dfm_weight(dfm_period, scheme = "prop")

term_prop <- convert(dfm_prop, to = "data.frame") %>%
  select(-doc_id) %>%
  mutate(period = rownames(dfm_prop))

term_long <- term_prop %>%
  pivot_longer(-period, names_to = "term", values_to = "prop")

term_wide <- term_long %>%
  pivot_wider(names_from = period, values_from = prop, values_fill = 0) %>%
  mutate(
    early_per_million = `1990-2004` * 1e6,
    late_per_million = `2005-2026` * 1e6,
    log2_ratio = log2((late_per_million + 1) / (early_per_million + 1))
  )

top_contrast <- bind_rows(
  head(arrange(term_wide, desc(log2_ratio)), 15),
  head(arrange(term_wide, log2_ratio), 15)
) %>%
  mutate(direction = ifelse(log2_ratio > 0, "Later period", "Earlier period"))

top_contrast %>% select(term, log2_ratio, direction)
```

```{r checkpoint-save-secondpass-analysis, cache.lazy=FALSE, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
# Checkpoint save (Second-pass analysis outputs)
save_checkpoint(top_contrast, "secondpass_key_terms")
```

## Model creation (second-pass embeddings)

We repeat the embedding approach on the second-pass corpus to identify framing shifts inside this narrower, more specific dataset. This provides a robustness check against the broader first-pass collection.

The second-pass embeddings are trained on a more curated corpus, which reduces noise and can sharpen semantic neighborhoods. Methodologically, this allows us to test whether the framing shifts observed in the first iteration persist when the dataset is more selective. We apply the same windowing strategy and caching logic to keep the comparison consistent. As before, we interpret neighbor changes as shifts in contextual association rather than stable meanings. This step therefore complements the first-pass embeddings and strengthens the overall argument.

We run a second, focused embedding pass on the filtered second-pass corpus. The goal is to surface terms that describe feminicide framing across 5-year windows.

```{r second-pass-word2vec-setup, cache.lazy=FALSE}
# Build 5-year windows based on available second-pass years
min_year_sp <- min(media_data_second$year, na.rm = TRUE)
max_year_sp <- max(media_data_second$year, na.rm = TRUE)
window_starts <- seq(floor(min_year_sp / 5) * 5, max_year_sp, by = 5)
time_windows_secondpass <- setNames(
  lapply(window_starts, function(y) c(y, min(y + 4, max_year_sp))),
  paste0(window_starts, "-", pmin(window_starts + 4, max_year_sp))
)

seed_terms_secondpass <- unique(target_map$normalized)
```

```{r second-pass-word2vec-training, cache=FALSE}
# Train one model per window for the second-pass corpus.
# Always export plain vectors (no external pointers) for reuse.
existing_vectors <- latest_checkpoint("word2vec_vectors_secondpass")
if (!RUN_SECOND_PASS_W2V_TRAINING) {
  message("Second-pass Word2Vec training disabled (RUN_SECOND_PASS_W2V_TRAINING = FALSE).")
  if (!is.null(existing_vectors)) {
    load_checkpoint("word2vec_vectors_secondpass", "word2vec_vectors_secondpass")
  }
} else if (!FORCE_SECOND_PASS_W2V_TRAINING && !is.null(existing_vectors)) {
  message("Second-pass Word2Vec vectors already exist; skipping training.")
  load_checkpoint("word2vec_vectors_secondpass", "word2vec_vectors_secondpass")
} else {
  word2vec_models_secondpass <- list()

  for (window_name in names(time_windows_secondpass)) {
    window_years <- time_windows_secondpass[[window_name]]
    window_corpus <- corpus_subset(
      secondpass_corpus,
      year >= window_years[1] & year <= window_years[2]
    )
    model <- train_w2v(window_name, window_corpus)
    if (!is.null(model)) {
      word2vec_models_secondpass[[window_name]] <- model
    }
  }

  word2vec_vectors_secondpass <- lapply(word2vec_models_secondpass, as.matrix)
  save_checkpoint(word2vec_vectors_secondpass, "word2vec_vectors_secondpass")
}
```

```{r checkpoint-load-word2vec-secondpass, cache.lazy=FALSE, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
# Checkpoint load (Second-pass Word2Vec vectors)
load_checkpoint("word2vec_vectors_secondpass", "word2vec_vectors_secondpass")
```

## Embedding diagnostics (second-pass)

We compute nearest neighbors for the curated target list and then visualize semantic movement over time in an aligned 3D space. Alignment is essential because embedding spaces are only identifiable up to rotation. By aligning windows to a shared reference, we can compare relative positions across time without conflating rotation artifacts with semantic change. The visualization is therefore a guided exploration of how specific targets and their neighbors move over time. We keep the point cloud small and label targets to make the plot interpretable. The aim is to provide an intuitive companion to the neighbor tables and to support the narrative of framing change.

```{r second-pass-word2vec-neighbors, cache.lazy=FALSE}
# Collect nearest neighbors for seed terms across windows
load_checkpoint("word2vec_vectors_secondpass", "word2vec_vectors_secondpass")

collect_neighbors_vectors <- function(vectors, seeds, top_n = 30) {
  seeds_in_vocab <- intersect(seeds, rownames(vectors))
  if (length(seeds_in_vocab) == 0) return(tibble())

  norms <- sqrt(rowSums(vectors^2))
  norms[norms == 0] <- NA_real_
  vectors_norm <- vectors / norms

  res <- lapply(seeds_in_vocab, function(seed) {
    seed_vec <- vectors_norm[seed, , drop = FALSE]
    sims <- as.numeric(vectors_norm %*% t(seed_vec))
    names(sims) <- rownames(vectors_norm)
    sims <- sims[names(sims) != seed]
    top <- sort(sims, decreasing = TRUE)[seq_len(min(top_n, length(sims)))]
    data.frame(
      seed = seed,
      term = names(top),
      similarity = as.numeric(top),
      rank = seq_along(top),
      stringsAsFactors = FALSE
    )
  })
  dplyr::bind_rows(res)
}

if (is.null(word2vec_vectors_secondpass)) {
  stop("No word2vec vectors available. Run the training chunk once.")
}

neighbors_secondpass <- dplyr::bind_rows(lapply(names(word2vec_vectors_secondpass), function(w) {
  vectors <- word2vec_vectors_secondpass[[w]]
  df <- collect_neighbors_vectors(vectors, seed_terms_secondpass, top_n = 30)
  df$window <- w
  df
}))

save_checkpoint(neighbors_secondpass, "word2vec_neighbors_secondpass")

candidate_terms_secondpass <- neighbors_secondpass %>%
  group_by(term) %>%
  summarise(
    mean_similarity = mean(similarity),
    seed_hits = n_distinct(seed),
    window_hits = n_distinct(window),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_similarity)) %>%
  filter(seed_hits >= 2, window_hits >= 2) %>%
  slice_head(n = 50)

candidate_terms_secondpass
```

```{r second-pass-word2vec-3d, cache.lazy=FALSE}
# We align spaces across windows before projecting to 3D. Without alignment,
# apparent movement may be an artifact of arbitrary embedding rotations.
# Interactive 3D view across time windows (aligned spaces)
# Important: align vectors across windows before comparing positions.
if (is.null(word2vec_vectors_secondpass)) {
  stop("No word2vec vectors available. Run the training chunk once.")
}

# Controls for density and focus
TOP_NEIGHBORS_PER_TARGET <- 10
MAX_MOVERS <- 30
MAX_POINTS_PER_WINDOW <- 250
MAX_TARGETS <- length(target_words)  # reduce for clarity if needed

# Select neighbors per target and window
targets_focus <- head(target_words, MAX_TARGETS)
targets_focus_norm <- normalize_terms_for_model(targets_focus)
target_label_map <- target_map %>%
  select(original, normalized) %>%
  distinct()
neighbors_top <- neighbors_secondpass %>%
  filter(seed %in% targets_focus_norm) %>%
  group_by(window, seed) %>%
  slice_max(order_by = similarity, n = TOP_NEIGHBORS_PER_TARGET, with_ties = FALSE) %>%
  ungroup()

# Dominant target per term/window (for coloring)
dominant_seed <- neighbors_top %>%
  group_by(window, term) %>%
  slice_max(order_by = similarity, n = 1, with_ties = FALSE) %>%
  ungroup() %>%
  select(window, term, seed)

# Build a compact term set: targets + top neighbors
terms_all <- unique(c(targets_focus_norm, neighbors_top$term))
terms_all <- terms_all[!is.na(terms_all)]

vectors_list <- word2vec_vectors_secondpass
windows <- names(vectors_list)
if (length(windows) < 2) {
  stop("Need at least two time windows for an interactive timeline.")
}

# Reference window for alignment
ref_window <- windows[1]
ref_vectors <- vectors_list[[ref_window]]

# Common terms used for alignment (must exist in all windows)
common_terms <- Reduce(intersect, lapply(vectors_list, rownames))
common_terms <- intersect(common_terms, rownames(ref_vectors))
if (length(common_terms) < 50) {
  message("WARN: Few common terms for alignment (", length(common_terms), ").")
}

align_to_reference <- function(vec, ref_vec, anchor_terms) {
  anchors <- intersect(anchor_terms, rownames(vec))
  anchors <- intersect(anchors, rownames(ref_vec))
  if (length(anchors) < 10) return(vec)
  A <- ref_vec[anchors, , drop = FALSE]
  B <- vec[anchors, , drop = FALSE]
  M <- t(B) %*% A
  sv <- svd(M)
  R <- sv$u %*% t(sv$v)
  vec %*% R
}

# Align all windows to reference
aligned_list <- lapply(vectors_list, function(v) {
  align_to_reference(v, ref_vectors, common_terms)
})

# PCA basis from reference window on shared terms (for comparable axes)
ref_anchor_terms <- intersect(terms_all, rownames(aligned_list[[ref_window]]))
ref_anchor_terms <- intersect(ref_anchor_terms, common_terms)
if (length(ref_anchor_terms) < 10) {
  stop("Not enough shared terms for stable PCA alignment.")
}
ref_mat <- aligned_list[[ref_window]][ref_anchor_terms, , drop = FALSE]
pca_ref <- prcomp(ref_mat, center = TRUE, scale. = FALSE)

project_window <- function(vectors, terms, pca_obj) {
  terms <- intersect(terms, rownames(vectors))
  mat <- vectors[terms, , drop = FALSE]
  coords <- scale(mat, center = pca_obj$center, scale = FALSE) %*% pca_obj$rotation[, 1:3]
  data.frame(
    term = terms,
    x = coords[, 1],
    y = coords[, 2],
    z = coords[, 3],
    stringsAsFactors = FALSE
  )
}

plot_data <- dplyr::bind_rows(lapply(windows, function(w) {
  vectors <- aligned_list[[w]]
  df <- project_window(vectors, terms_all, pca_ref)
  df$window <- w
  df$type <- ifelse(df$term %in% seed_terms_secondpass, "target", "neighbor")
  df
}))

# Track movers: terms with largest displacement across windows
if (length(windows) >= 2) {
  mover_candidates <- plot_data %>%
    group_by(term) %>%
    filter(n() == length(windows)) %>%
    summarise(
      dx = max(x) - min(x),
      dy = max(y) - min(y),
      dz = max(z) - min(z),
      displacement = sqrt(dx^2 + dy^2 + dz^2),
      .groups = "drop"
    ) %>%
    arrange(desc(displacement)) %>%
    slice_head(n = MAX_MOVERS) %>%
    pull(term)
} else {
  mover_candidates <- character(0)
}

plot_data <- plot_data %>%
  left_join(dominant_seed, by = c("window", "term")) %>%
  left_join(target_label_map, by = c("term" = "normalized")) %>%
  mutate(
    is_mover = term %in% mover_candidates,
    label = ifelse(type == "target", original, term)
  )

# Keep: all targets + top neighbors + movers
plot_data <- plot_data %>%
  filter(type == "target" | !is.na(seed) | is_mover)

# Cap total points per window
plot_data <- plot_data %>%
  group_by(window) %>%
  slice_head(n = MAX_POINTS_PER_WINDOW) %>%
  ungroup()

# Split targets vs neighbors for clearer rendering and stable traces
targets_plot <- plot_data %>% filter(type == "target")
neighbors_plot <- plot_data %>% filter(type == "neighbor")

p_w2v_3d <- plotly::plot_ly() %>%
  plotly::add_trace(
    data = neighbors_plot,
    x = ~x, y = ~y, z = ~z,
    frame = ~window,
    type = "scatter3d",
    mode = "markers",
    marker = list(size = 3, opacity = 0.5, color = "gray70"),
    text = ~label,
    hoverinfo = "text",
    name = "Neighbors"
  ) %>%
  plotly::add_trace(
    data = targets_plot,
    x = ~x, y = ~y, z = ~z,
    frame = ~window,
    type = "scatter3d",
    mode = "markers+text",
    marker = list(size = 7, opacity = 0.95, color = "crimson"),
    text = ~label,
    textposition = "top center",
    hoverinfo = "text",
    name = "Targets"
  ) %>%
  plotly::layout(
    title = "Word2Vec 3D timeline (aligned windows)",
    scene = list(
      xaxis = list(title = "PC1"),
      yaxis = list(title = "PC2"),
      zaxis = list(title = "PC3")
    ),
    legend = list(orientation = "h")
  )

p_w2v_3d
```

# Analysis and discussion

This section synthesizes the empirical findings and situates them in relation to the guiding research interest: how lethal violence against women is linguistically framed in German-language Swiss media over time, and how shifts in naming practices become visible—or remain invisible—within large-scale text data. The aim is not to establish causal explanations or definitive classifications, but to interpret observed patterns as indicators of changing discursive conditions under which violence becomes sayable, searchable, and countable.

## Discursive shifts in naming gendered violence

Across descriptive frequency trends and distributional analyses, we observe a gradual but uneven reconfiguration of how lethal violence against women is named. Euphemistic or relational framings such as *Familiendrama*, *Familientragödie*, *Beziehungstat* or *Ehestreit* decline in relative prominence over time, while more explicit terms gain visibility in later periods.

This shift, however, requires differentiation. *Frauenmord* is not a new or recently politicized term; it has long been present in German-language reporting and legal discourse. Its continued use therefore does not, in itself, indicate a feminist reframing. By contrast, *Femizid* appears with increasing frequency from the mid-2010s onward and is more clearly connected to international debates on gendered violence. *Feminizid*, which explicitly foregrounds structural power relations and patriarchal violence, remains marginal in German-Swiss media and has not (yet) become part of a broadly established journalistic vocabulary.

Rather than a linear replacement of older terms by newer ones, the data points to a layered and heterogeneous discourse. Different framings coexist across outlets, genres, and temporal moments. Media discourse mobilizes multiple linguistic registers to narrate similar acts of violence, reflecting competing interpretive frameworks rather than a unified shift.

The distributional analysis using distributional word embeddings trained with the Word2Vec algorithm supports this interpretation. Following the distributional hypothesis, these models capture patterns of co-occurrence rather than fixed meanings. Over time, *Femizid* increasingly appears in semantic neighborhoods associated with public debate, responsibility, and structural violence, whereas euphemistic terms remain embedded in contexts of interpersonal conflict, tragedy, or affective explanation. These shifts indicate changing contextual associations, not stable semantic redefinitions.

Crucially, these findings should be read as discursive indicators. They speak to how violence is narrated and framed in media discourse, not to changes in the incidence, causes, or lived realities of violence itself.

## Euphemistic versus explicit framing over time

The contrast between euphemistic and explicit naming becomes particularly visible when aggregated into a simplified time series. @framing-shift-summary contrasts normalized frequencies of selected euphemistic framings with explicit naming practices per million words over time.

While absolute frequencies fluctuate with overall media volume and event-driven attention, the relative trajectories suggest a declining reliance on euphemistic framings alongside a gradual increase in explicit naming.

```{r}
# | label: framing-shift-summary
# | fig-cap: Euphemistic versus explicit framing over time (frequency per million words). The plot contrasts aggregated euphemistic framings (e.g. *Familiendrama*, *Beziehungstat*) with explicit naming (e.g. *Femizid*, *Feminizid*, *Frauenmord*).
euphemistic_terms <- c("Familiendrama", "Familientragödie", "Beziehungstat", "Ehestreit")
explicit_terms <- c("Femizid", "Feminizid", "Frauenmord")

media_terms <- media_data %>%
  filter(!is.na(content), content != "") %>%
  mutate(
    year = as.numeric(year),
    content_lower = stringr::str_to_lower(content),
    word_count = stringr::str_count(content_lower, "\\S+")
  ) %>%
  filter(!is.na(year), year >= 2000, year <= 2025)

total_words_year <- media_terms %>%
  group_by(year) %>%
  summarise(total_words = sum(word_count), .groups = "drop")

make_term_regex <- function(term) {
  term <- stringr::str_to_lower(term)
  parts <- strsplit(term, "\\s+")[[1]]
  paste0("\\b", paste(parts, collapse = "\\s+"), "\\b")
}

count_terms <- function(terms, label) {
  counts <- dplyr::bind_rows(lapply(terms, function(term) {
    pattern <- make_term_regex(term)
    media_terms %>%
      mutate(count = stringr::str_count(content_lower, pattern)) %>%
      group_by(year) %>%
      summarise(count = sum(count), .groups = "drop")
  }))
  counts %>%
    group_by(year) %>%
    summarise(count = sum(count), .groups = "drop") %>%
    mutate(group = label)
}

group_counts <- bind_rows(
  count_terms(euphemistic_terms, "Euphemistic framing"),
  count_terms(explicit_terms, "Explicit naming")
) %>%
  left_join(total_words_year, by = "year") %>%
  mutate(freq_per_million = (count / total_words) * 1e6)

p_framing_shift <- ggplot(group_counts, aes(x = year, y = freq_per_million, color = group)) +
  geom_line(linewidth = 1.1, alpha = 0.9) +
  theme_minimal() +
  labs(
    title = "Euphemistic vs explicit framing over time",
    subtitle = "Frequency per million words",
    x = "Year", y = "Frequency per million words", color = NULL
  ) +
  theme(legend.position = "bottom")

p_framing_shift
```

## Visibility, absence, and epistemic limits

While the observed trends point toward increased explicitness in some parts of media discourse, a central limitation of this analysis lies in what cannot be made visible through the data. The corpus consists exclusively of media reporting indexed in Swissdox. Acts of lethal violence against women that were not reported, were reported only marginally, or were linguistically attenuated beyond recognition remain structurally absent.

From a feminist data perspective, this absence is not incidental. As argued by @dignazio_klein_data_feminism_2020, data infrastructures do not neutrally represent reality; they actively shape what becomes visible, legible, and countable. In the context of feminicide, the lack of official statistics and the reliance on activist counting practices underscore that visibility itself is politically contested. What enters the corpus is not feminicide as such, but media-recognized and linguistically articulated instances of violence.

This has direct implications for interpretation. Killings framed without gender markers, relational descriptors, or recognizable violence terms may never enter the analytical space. In this sense, the corpus captures discursive recognition rather than empirical prevalence. The silences in the data are therefore analytically meaningful, even though they cannot be directly quantified.

A further limitation concerns event-driven distortions. High-profile cases can dominate coverage and skew term distributions. In our dataset, repeated references to *Hochseilartist* trace back to extensive reporting on the Freddy-Nock case, which ultimately ended in an acquittal [@noauthor_toxische_2020]. Such cases illustrate how individual media events can introduce semantic noise that is unrelated to feminicide discourse as such.

## Exploratory intent and methodological reflexivity

It is therefore essential to emphasize the exploratory character of this project. We do not claim to have identified a definitive vocabulary, an exhaustive corpus, or a stable typology of framings. Especially in the first iteration, the central aim was term discovery: exploring how different ways of naming cluster, overlap, or remain isolated within the corpus.

The use of Word2Vec embeddings was instrumental in this regard. By modeling distributional similarity rather than predefined categories, the method led us toward blind spots and unexpected associations that challenged the initial keyword set. Some suggested neighbors proved analytically productive; others revealed ambiguity, noise, or misleading overlaps. Both outcomes are valuable, as they expose the limits of intuition-driven keyword selection and foreground the interpretive work required in computational discourse analysis.

We deliberately refrain from drawing strong or conclusive claims. Given the ethical and political stakes of researching gendered violence, epistemic modesty is warranted. The contribution of this study lies less in its substantive findings than in its process: demonstrating how computational methods can be used reflexively to explore discursive change, identify blind spots, and foreground the politics of visibility in large-scale text data. Rather than offering closure, the analysis opens further questions about how gendered violence is named, normalized, or silenced—and how such processes might be studied more rigorously in future work.

# References {.unnumbered}

::: {#refs}
:::