---
title: "Feminizide in Medien - Datenerhebung"
output: html_notebook
---

# Setup

```{r setup, include=FALSE}
library(httr)
library(jsonlite)
library(yaml)
library(dotenv)
library(R6)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidytext)
library(readr)
library(tidyr)
library(quanteda)
library(quanteda.textplots)
library(tm)
library(topicmodels)
library(wordcloud)
library(RColorBrewer)

# API Wrapper laden
source("swissdox_api.R")
```

# Datenerhebung: Feminizide und Familiendrama

```{r api-setup}
# Environment-Variablen aus local.env laden
if (file.exists("local.env")) {
  load_dot_env("local.env")
  message("\u2713 Environment-Variablen aus local.env geladen")
} else {
  message("WARN: local.env nicht gefunden - verwende System-Environment")
}

# API Client mit Credentials initialisieren
client <- SwissdoxAPI$new(
  api_key = Sys.getenv("SWISSDOX_API_KEY"),
  api_secret = Sys.getenv("SWISSDOX_API_SECRET")
)
```

```{r checkpoint-utils}
# Checkpoint helpers (load/save latest timestamped files)
find_latest <- function(pattern, dir = "data") {
  files <- list.files(dir, pattern = pattern, full.names = TRUE)
  if (length(files) == 0) return(NULL)
  files[order(file.mtime(files), decreasing = TRUE)][1]
}

latest_checkpoint <- function(prefix, dir = "data") {
  pattern <- paste0("^", prefix, "_\\d{8}_\\d{6}\\.rds$")
  find_latest(pattern, dir)
}

save_checkpoint <- function(obj, prefix, dir = "data", compress = "xz") {
  file <- file.path(dir, paste0(prefix, "_", format(Sys.time(), "%Y%m%d_%H%M%S"), ".rds"))
  saveRDS(obj, file, compress = compress)
  message("Saved checkpoint: ", basename(file))
  file
}

load_checkpoint <- function(prefix, var_name) {
  file <- latest_checkpoint(prefix)
  if (is.null(file)) {
    message("No checkpoint found for: ", prefix)
    return(NULL)
  }
  assign(var_name, readRDS(file), envir = .GlobalEnv)
  message("Loaded checkpoint: ", basename(file))
  file
}
```

```{r text-config}
# Text analysis defaults
SAVE_CORPUS <- FALSE
USE_STEMMING <- TRUE
UMLAUT_MODE <- "none"  # "none" | "ae" | "strip"

# Multi-word patterns after stemming (used in preprocessing + word2vec)
simplified_patterns <- list(
  c("hauslich", "gewalt"),
  c("partnerschaft", "gewalt"),
  c("partner", "gewalt"),
  c("bezieh", "gewalt"),
  c("sexuell", "gewalt"),
  c("k\u00f6rperl", "gewalt"),
  c("psychisch", "gewalt"),
  c("t\u00f6t", "frau"),
  c("mord", "frau"),
  c("gewalt", "frau"),
  c("famili", "drama"),
  c("famili", "trag\u00f6di"),
  c("bezieh", "tat"),
  c("eh", "streit"),
  c("opf", "schutz"),
  c("gewalt", "schutz")
)

# Normalize terms to match Word2Vec vocabulary (lowercase, optional umlaut, optional stemming)
normalize_terms_for_model <- function(terms) {
  norm <- stringr::str_to_lower(terms)
  if (UMLAUT_MODE == "ae") {
    norm <- stringr::str_replace_all(
      norm,
      c("\u00e4" = "ae", "\u00f6" = "oe", "\u00fc" = "ue", "\u00df" = "ss")
    )
  } else if (UMLAUT_MODE == "strip") {
    norm <- stringr::str_replace_all(
      norm,
      c("\u00e4" = "a", "\u00f6" = "o", "\u00fc" = "u", "\u00df" = "ss")
    )
  }
  toks <- quanteda::tokens(norm, remove_punct = TRUE)
  if (USE_STEMMING) {
    toks <- quanteda::tokens_wordstem(toks, language = "de")
  }
  vapply(toks, function(x) paste(x, collapse = "_"), character(1))
}

feminizid_keywords <- c(
  "Feminizid", "Femizid", "Frauenmord", "Femicide",
  "Familiendrama", "Familientrag\u00f6die", "Beziehungstat", "Ehestreit",
  "h\u00e4usliche Gewalt", "Partnerschaftsgewalt", "Gewalt gegen Frauen",
  "T\u00f6tung von Frau", "Mord an Frau", "Partnergewalt", "Beziehungsgewalt"
)
```

## Data collection

Build and run the Swissdox query. Use the checkpoint load block to resume
without re-running the API call.

```{r checkpoint-load-raw}
# Checkpoint load (raw data)
load_checkpoint("feminizid_raw_data", "media_data")
```

```{r feminizid-data-collection}
# Vollst\u00e4ndige Datenerhebung mit API-Wrapper
collection_result <- execute_swissdox_workflow(
  client = client,
  query = create_swissdox_query(
    content_terms = feminizid_keywords,
    date_from = "2000-01-01",
    date_to = format(Sys.Date(), "%Y-%m-%d"),
    languages = "de",
    max_results = 1000000L,
    columns = c("id", "pubtime", "medium_code", "medium_name", "rubric",
               "regional", "doctype", "doctype_description", "language", 
               "char_count", "dateline", "head", "subhead", 
               "article_link", "content_id", "content")
  ),
  name = paste("Feminizid_Collection", format(Sys.time(), "%Y%m%d_%H%M%S")),
  comment = "Comprehensive feminicide media analysis dataset",
  return_data = TRUE,
  output_dir = "data"
)

# Store results and save checkpoint
if (!is.null(collection_result$data)) {
  media_data <- collection_result$data
  save_checkpoint(media_data, "feminizid_raw_data")
  message("\u2713 ", collection_result$message)
} else {
  message("Fehler beim Laden der Daten")
}
```

## Data preparation

Load raw data (from checkpoint or TSV), clean fields, and prepare time
variables used in analysis.

```{r data-preparation}
# Load data (checkpoint first, TSV fallback)
if (!exists("media_data")) {
  raw_file <- load_checkpoint("feminizid_raw_data", "media_data")
  if (is.null(raw_file)) {
    tsv_files <- list.files("data", pattern = "\\.tsv(\\.xz)?$", full.names = TRUE)
    if (length(tsv_files) == 0) {
      message("WARN: Keine Datendateien gefunden - f\u00fchren Sie API-Call aus")
    } else {
      message("Keine .rds Datei gefunden, lade .tsv Dateien...")
      media_data <- bind_rows(lapply(tsv_files, function(file) {
        message("Lade: ", basename(file))
        read_tsv(file, locale = locale(encoding = "UTF-8"))
      })) %>% distinct(id, .keep_all = TRUE)
      message("\u2713 ", length(tsv_files), " Dateien kombiniert: ", nrow(media_data), " Artikel")
      save_checkpoint(media_data, "feminizid_raw_data")
    }
  }
}
```

```{r data-cleaning}

# Daten aufbereiten (falls noch nicht geschehen)
if (exists("media_data")) {
  # Datum konvertieren und zus\u00e4tzliche Zeitvariablen erstellen
  media_data$pubtime <- as.POSIXct(media_data$pubtime)
  media_data$pub_date <- as.Date(media_data$pubtime)
  media_data$year <- format(media_data$pub_date, "%Y")
  media_data$month <- format(media_data$pub_date, "%m")
  media_data$month_year <- format(media_data$pub_date, "%Y-%m")
  
  # Nur deutsche Artikel (franz\u00f6sische rausfiltern)
  media_data <- media_data %>%
    filter(language == "de") %>%
    filter(!is.na(content), content != "", nchar(content) > 50)
  
  message("Deutsche Artikel nach Filterung: ", nrow(media_data))
  message("Zeitraum: ", min(media_data$pub_date, na.rm = TRUE), " bis ", max(media_data$pub_date, na.rm = TRUE))
  message("Medien: ", length(unique(media_data$medium_name)))
} else {
  message("Keine Daten verf\u00fcgbar - f\u00fchren Sie zuerst die Datenerhebung aus")
}
```

```{r overview-analysis}
if (exists("media_data")) {
  
  # 1. Artikel pro Jahr
  yearly_counts <- media_data %>%
    count(year) %>%
    filter(!is.na(year), year >= 2000, year <= 2026)
  
  p1 <- ggplot(yearly_counts, aes(x = as.numeric(year), y = n)) +
    geom_col(fill = "steelblue", alpha = 0.7) +
    theme_minimal() +
    labs(title = "Feminizid-Berichterstattung \u00fcber Zeit", 
         subtitle = "Deutsche Medien 2000-2026",
         x = "Jahr", y = "Anzahl Artikel") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # 2. Top Medien
  top_media <- media_data %>%
    count(medium_name, sort = TRUE) %>%
    head(15)
  
  p2 <- ggplot(top_media, aes(x = reorder(medium_name, n), y = n)) +
    geom_col(fill = "coral", alpha = 0.7) +
    coord_flip() +
    theme_minimal() +
    labs(title = "Top 15 Medien", x = "Medium", y = "Anzahl Artikel")
  
  # 3. Monatlicher Trend (letzte 5 Jahre)
  recent_trend <- media_data %>%
    filter(year >= 2020) %>%
    count(month_year) %>%
    arrange(month_year)
  
  p3 <- ggplot(recent_trend, aes(x = month_year, y = n)) +
    geom_line(group = 1, color = "darkgreen", size = 1) +
    geom_point(color = "darkgreen", size = 1.5) +
    theme_minimal() +
    labs(title = "Monatlicher Trend 2020-2026", x = "Monat", y = "Anzahl Artikel") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))
  
  print(p1)
  print(p2) 
  print(p3)
  
  # Zusammenfassung
  cat("\n=== DATEN\u00dcBERSICHT ===\n")
  cat("Deutsche Artikel:", nrow(media_data), "\n")
  cat("Zeitraum:", min(media_data$pub_date, na.rm = TRUE), "bis", max(media_data$pub_date, na.rm = TRUE), "\n")
  cat("Medien:", length(unique(media_data$medium_name)), "\n")
  cat("Durchschnitt/Jahr:", round(nrow(media_data) / length(unique(media_data$year))), "\n")
}
```

```{r keyword-analysis}
if (exists("media_data")) {
  
  # Keyword-H\u00e4ufigkeit in Headlines (verwende zentrale Definition)
  keyword_counts <- data.frame(keyword = character(), count = numeric())
  
  for (kw in feminizid_keywords) {
    count <- sum(str_detect(media_data$head, regex(kw, ignore_case = TRUE)), na.rm = TRUE)
    keyword_counts <- rbind(keyword_counts, data.frame(keyword = kw, count = count))
  }
  
  keyword_counts <- keyword_counts %>%
    filter(count > 0) %>%
    arrange(desc(count))
  
  p4 <- ggplot(keyword_counts, aes(x = reorder(keyword, count), y = count)) +
    geom_col(fill = "darkred", alpha = 0.7) +
    coord_flip() +
    theme_minimal() +
    labs(title = "Keyword-H\u00e4ufigkeit in Schlagzeilen", 
         x = "Keyword", y = "Anzahl Artikel")
  
  print(p4)
  
  # Top Keywords ausgeben
  cat("\n=== KEYWORD-H\u00c4UFIGKEIT ===\n")
  print(keyword_counts)
}
```

```{r time-trends}
if (exists("media_data")) {
  
  # Zeittrends f\u00fcr Keywords (verwende zentrale Definition)
  keyword_timeline <- data.frame()
  
  for (kw in feminizid_keywords) {
    yearly_kw <- media_data %>%
      filter(!is.na(year), !is.na(head)) %>%
      mutate(has_keyword = str_detect(head, regex(kw, ignore_case = TRUE))) %>%
      group_by(year) %>%
      summarise(
        total_articles = n(),
        keyword_articles = sum(has_keyword, na.rm = TRUE),
        percentage = (keyword_articles / total_articles) * 100,
        .groups = 'drop'
      ) %>%
      mutate(keyword = kw)
    
    keyword_timeline <- bind_rows(keyword_timeline, yearly_kw)
  }
  
  # Nur relevante Keywords und Jahre
  keyword_timeline <- keyword_timeline %>%
    filter(year >= 2000, year <= 2026) %>%
    group_by(keyword) %>%
    filter(sum(keyword_articles) > 10) %>%  # Mindestens 10 Artikel insgesamt
    ungroup()
  
  # Absolute Zahlen \u00fcber Zeit
  p5 <- ggplot(keyword_timeline, aes(x = as.numeric(year), y = keyword_articles, color = keyword)) +
    geom_line(size = 1, alpha = 0.8) +
    geom_point(size = 1.5, alpha = 0.7) +
    theme_minimal() +
    labs(title = "Zeittrends: Keyword-Verwendung in Schlagzeilen",
         subtitle = "Absolute Anzahl Artikel pro Jahr",
         x = "Jahr", y = "Anzahl Artikel", color = "Keyword") +
    theme(legend.position = "bottom", legend.text = element_text(size = 8)) +
    guides(color = guide_legend(ncol = 2))
  
  # Heatmap: Keywords x Jahre
  keyword_matrix <- keyword_timeline %>%
    select(year, keyword, keyword_articles) %>%
    pivot_wider(names_from = keyword, values_from = keyword_articles, values_fill = 0) %>%
    pivot_longer(-year, names_to = "keyword", values_to = "count")
  
  p6 <- ggplot(keyword_matrix, aes(x = as.numeric(year), y = keyword, fill = count)) +
    geom_tile(color = "white", size = 0.1) +
    scale_fill_gradient(low = "white", high = "darkred", name = "Artikel") +
    theme_minimal() +
    labs(title = "Keyword-Intensit\u00e4t \u00fcber Zeit", x = "Jahr", y = "Keyword") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(size = 9))
  
  print(p5)
  print(p6)
  
  # Wichtigste Trends
  cat("\n=== ZEITTREND-ANALYSE ===\n")
  trend_summary <- keyword_timeline %>%
    group_by(keyword) %>%
    summarise(
      total_articles = sum(keyword_articles),
      peak_year = year[which.max(keyword_articles)],
      peak_count = max(keyword_articles),
      .groups = 'drop'
    ) %>%
    arrange(desc(total_articles))
  
  print(trend_summary)
}
```

## Corpus and preprocessing

Build a quanteda corpus and a trimmed DFM from the cleaned articles. Use the
checkpoint load block to resume without reprocessing.

```{r checkpoint-load-corpus}
# Checkpoint load (corpus/dfm)
load_checkpoint("feminizid_corpus", "feminizid_corpus")
load_checkpoint("feminizid_dfm", "feminizid_dfm_trimmed")
```

```{r corpus-setup}
stopifnot(exists("media_data"))

if (!exists("feminizid_corpus")) {
  corpus_data <- media_data %>%
    distinct(id, .keep_all = TRUE) %>%
    filter(!is.na(content), content != "", nchar(content) > 100) %>%
    mutate(unique_id = paste0("doc_", row_number()))

  message("Articles for corpus: ", nrow(corpus_data))

  corpus_data$clean_content <- corpus_data$content %>%
    str_remove_all("<[^>]+>") %>%
    str_remove_all("\\n|\\t|\\r") %>%
    str_remove_all("https?://\\S+") %>%
    str_squish()

  # Optional umlaut normalization (affects stemming + word2vec)
  if (UMLAUT_MODE == "ae") {
    corpus_data$clean_content <- str_replace_all(
      corpus_data$clean_content,
      c("\u00e4" = "ae", "\u00f6" = "oe", "\u00fc" = "ue", "\u00df" = "ss")
    )
  } else if (UMLAUT_MODE == "strip") {
    corpus_data$clean_content <- str_replace_all(
      corpus_data$clean_content,
      c("\u00e4" = "a", "\u00f6" = "o", "\u00fc" = "u", "\u00df" = "ss")
    )
  }

  corpus_data <- corpus_data %>%
    filter(!is.na(clean_content), clean_content != "", nchar(clean_content) > 50)

  feminizid_corpus <- corpus(corpus_data,
                             text_field = "clean_content",
                             docid_field = "unique_id")

  docvars(feminizid_corpus, "original_id") <- corpus_data$id
  docvars(feminizid_corpus, "medium") <- corpus_data$medium_name
  docvars(feminizid_corpus, "date") <- corpus_data$pub_date
  docvars(feminizid_corpus, "year") <- corpus_data$year
  docvars(feminizid_corpus, "headline") <- corpus_data$head
  docvars(feminizid_corpus, "char_count") <- as.numeric(corpus_data$char_count)

  cat("=== QUANTEDA CORPUS ===\n")
  cat("Documents:", ndoc(feminizid_corpus), "\n")
  cat("Tokens:", sum(ntoken(feminizid_corpus)), "\n")
  cat("Range:", min(corpus_data$pub_date, na.rm = TRUE), "to", max(corpus_data$pub_date, na.rm = TRUE), "\n")

  print(summary(feminizid_corpus, n = 3))

  save_checkpoint(feminizid_corpus, "feminizid_corpus")
}
```

```{r text-preprocessing}
stopifnot(exists("feminizid_corpus"))

# Deutsche Stoppw\u00f6rter laden
german_stopwords <- readLines("german_stopwords_full.txt", encoding = "UTF-8")
german_stopwords <- german_stopwords[german_stopwords != ""]

# Medienspezifische Stoppw\u00f6rter
media_stopwords <- c("uhr", "jahr", "jahre", "jahren", "prozent", "franken", "chf",
                    "artikel", "bericht", "zeitung", "medium", "redaktion", "autor",
                    "quelle", "foto", "bild", "video", "online", "www", "http",
                    "montag", "dienstag", "mittwoch", "donnerstag", "freitag",
                    "samstag", "sonntag", "januar", "februar", "m\u00e4rz", "april",
                    "mai", "juni", "juli", "august", "september", "oktober",
                    "november", "dezember", "schweiz", "schweizer", "schweizerin")

all_stopwords <- c(german_stopwords, media_stopwords)


# Tokenisierung und Preprocessing
feminizid_tokens <- tokens(feminizid_corpus,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_numbers = TRUE,
                          remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(all_stopwords, min_nchar = 3)

# Stemming (optional)
if (USE_STEMMING) {
  feminizid_tokens <- tokens_wordstem(feminizid_tokens, language = "de")
}

# DANN Multi-Word-Expressions (jetzt mit Grundformen)
# Vereinfachte Patterns - nur Grundformen n\u00f6tig
# Uses simplified_patterns defined in text-config

# Multi-Word-Expressions zu Compound-Tokens zusammenfassen
for (pattern in simplified_patterns) {
  feminizid_tokens <- tokens_compound(feminizid_tokens, 
                                      pattern = phrase(paste(pattern, collapse = " ")),
                                      concatenator = "_")
}

cat("=== TEXT-PREPROCESSING SCHRITTE ===\n")
cat("1. ERST Stemming: Flexionsformen reduziert\n")
cat("   h\u00e4usliche/h\u00e4uslicher \u2192 h\u00e4usl, t\u00f6tung \u2192 t\u00f6t, frauen \u2192 frau\n")
cat("2. DANN Multi-Word-Expressions:", length(simplified_patterns), "Wortpaare\n")
cat("   h\u00e4usl + gewalt \u2192 h\u00e4usl_gewalt (erfasst ALLE Flexionsformen!)\n")
cat("3. Vorteil: Ein Pattern erfasst alle Varianten!\n")

# Document-Feature-Matrix
feminizid_dfm <- dfm(feminizid_tokens)

# Trimming: Konsistente Parameter (beide als Proportionen)
# Basierend auf quanteda Dokumentation: https://quanteda.io/reference/dfm_trim.html
feminizid_dfm_trimmed <- dfm_trim(feminizid_dfm, 
                                  min_docfreq = 0.0003,  # Min. 0.03% (~11 Dokumente bei 35k)
                                  max_docfreq = 0.90,    # Max. 90% der Dokumente
                                  docfreq_type = "prop")  # Beide als Proportionen

cat("=== TEXT-PREPROCESSING ===\n")
cat("Dokumente:", ndoc(feminizid_dfm), "\n")
cat("Original Features:", nfeat(feminizid_dfm), "\n")
cat("Nach Trimming:", nfeat(feminizid_dfm_trimmed), "\n")
cat("Sparsity:", round(sparsity(feminizid_dfm_trimmed), 3), "\n")

# H\u00e4ufigste W\u00f6rter
if (nfeat(feminizid_dfm_trimmed) > 0) {
  top_features <- topfeatures(feminizid_dfm_trimmed, 25)
  cat("\n=== H\u00c4UFIGSTE W\u00d6RTER ===\n")
  print(top_features)
} else {
  cat("\nWARN: Keine Features nach Trimming - Parameter anpassen!\n")
}
```

```{r checkpoint-save-dfm}
# Checkpoint save (DFM and optional corpus)
if (exists("feminizid_dfm_trimmed") && nfeat(feminizid_dfm_trimmed) > 0) {
  save_checkpoint(feminizid_dfm_trimmed, "feminizid_dfm")
  if (exists("SAVE_CORPUS") && SAVE_CORPUS == TRUE && exists("feminizid_corpus")) {
    save_checkpoint(feminizid_corpus, "feminizid_corpus")
  }
}
```

```{r visualization}
# SMART VISUALIZATION: Auto-Skip wenn DFM fehlt
if (!exists("feminizid_dfm_trimmed")) {
  message("ERROR: Keine DFM - f\u00fchre erst text-preprocessing aus")
} else {
  message("Starte Visualisierungen...")
  
  # Wordcloud der h\u00e4ufigsten Begriffe
  set.seed(123)
  
  # Versuche quanteda.textplots, falls nicht verf\u00fcgbar verwende wordcloud
  if ("textplot_wordcloud" %in% ls("package:quanteda.textplots")) {
    textplot_wordcloud(feminizid_dfm_trimmed, 
                      min_count = 30,
                      max_words = 100,
                      color = brewer.pal(8, "Dark2"))
  } else {
    # Alternative mit wordcloud package
    freq_data <- topfeatures(feminizid_dfm_trimmed, 100)
    wordcloud(names(freq_data), freq_data, 
             min.freq = 30, max.words = 100,
             colors = brewer.pal(8, "Dark2"),
             random.order = FALSE)
  }
  
  # Feature-H\u00e4ufigkeit \u00fcber Zeit (normalisiert)
  if (exists("feminizid_corpus")) {
    # Top 10 Features nach Jahren
    top_terms <- names(topfeatures(feminizid_dfm_trimmed, 10))
    
    # H\u00e4ufigkeit dieser Terms \u00fcber Jahre
    yearly_terms <- dfm_group(feminizid_dfm_trimmed, groups = docvars(feminizid_dfm_trimmed, "year"))
    
    # Artikel pro Jahr f\u00fcr Normalisierung
    articles_per_year <- feminizid_corpus %>%
      docvars() %>%
      mutate(year = as.numeric(year)) %>%  # Jahr als numerisch konvertieren
      count(year, name = "total_articles") %>%
      filter(year >= 2000, year <= 2025)
    
    # Korrekte Konvertierung f\u00fcr quanteda 3.0+ - ohne doc_id Spalte
    term_data <- convert(yearly_terms[, top_terms], to = "data.frame") %>%
      select(-doc_id) %>%  # doc_id Spalte entfernen
      mutate(year = as.numeric(rownames(yearly_terms)))
    
    # Absolute H\u00e4ufigkeiten
    term_timeline_abs <- term_data %>%
      pivot_longer(-year, names_to = "term", values_to = "frequency") %>%
      filter(year >= 2000, year <= 2025)
    
    # Normalisierte H\u00e4ufigkeiten (pro 1000 Artikel)
    term_timeline_norm <- term_timeline_abs %>%
      left_join(articles_per_year, by = "year") %>%
      mutate(frequency_per_1000 = (frequency / total_articles) * 1000) %>%
      filter(!is.na(total_articles), total_articles > 10)  # Nur Jahre mit >10 Artikeln
    
    # Plot 1: Absolute H\u00e4ufigkeiten
    p7a <- ggplot(term_timeline_abs, aes(x = year, y = frequency, color = term)) +
      geom_line(size = 1, alpha = 0.8) +
      theme_minimal() +
      labs(title = "H\u00e4ufigste Begriffe \u00fcber Zeit (absolut)", 
           x = "Jahr", y = "Absolute H\u00e4ufigkeit", color = "Begriff") +
      theme(legend.position = "bottom")
    
    # Plot 2: Normalisierte H\u00e4ufigkeiten (pro 1000 Artikel)
    p7b <- ggplot(term_timeline_norm, aes(x = year, y = frequency_per_1000, color = term)) +
      geom_line(size = 1, alpha = 0.8) +
      theme_minimal() +
      labs(title = "H\u00e4ufigste Begriffe \u00fcber Zeit (normalisiert pro 1000 Artikel)", 
           x = "Jahr", y = "H\u00e4ufigkeit pro 1000 Artikel", color = "Begriff") +
      theme(legend.position = "bottom")
    
    # Plot 3: Artikel-Verteilung \u00fcber Zeit (zur Kontrolle)
    p7c <- ggplot(articles_per_year, aes(x = year, y = total_articles)) +
      geom_line(size = 1, color = "darkblue") +
      geom_point(size = 2, color = "darkblue") +
      theme_minimal() +
      labs(title = "Artikel-Verteilung \u00fcber Zeit", 
           x = "Jahr", y = "Anzahl Artikel") +
      scale_y_continuous(labels = scales::comma)
    
    print(p7a)
    print(p7b) 
    print(p7c)
    
    # Zusammenfassung der Normalisierung
    cat("\n=== NORMALISIERUNGS-ANALYSE ===\n")
    summary_stats <- articles_per_year %>%
      summarise(
        min_articles = min(total_articles),
        max_articles = max(total_articles),
        mean_articles = round(mean(total_articles)),
        median_articles = median(total_articles),
        ratio_max_min = round(max(total_articles) / min(total_articles), 1)
      )
    
    cat("Min Artikel pro Jahr:", summary_stats$min_articles, "\n")
    cat("Max Artikel pro Jahr:", summary_stats$max_articles, "\n")
    cat("Durchschnitt:", summary_stats$mean_articles, "\n")
    cat("Median:", summary_stats$median_articles, "\n")
    cat("Verh\u00e4ltnis Max/Min:", summary_stats$ratio_max_min, "x\n")
    
    if (summary_stats$ratio_max_min > 5) {
      cat("WARN: Starke Verzerrung erkannt - normalisierte Darstellung empfohlen!\n")
    }
  }
}
```

## Word2Vec frame shift (diachronic)

Train one Word2Vec model per time window and compare nearest neighbors
across periods. Use the checkpoint load block to resume without retraining.

```{r checkpoint-load-word2vec}
# Checkpoint load (Word2Vec)
load_checkpoint("word2vec_models", "word2vec_models")
load_checkpoint("word2vec_frame_shifts", "all_frame_shifts")
```

```{r word2vec-setup}
stopifnot(exists("feminizid_corpus"))
library(word2vec)
  
  # Zeitfenster definieren (5-Jahres-Fenster f\u00fcr ausreichend Daten)
  time_windows <- list(
    "2000-2004" = c(2000, 2004),
    "2005-2009" = c(2005, 2009), 
    "2010-2014" = c(2010, 2014),
    "2015-2019" = c(2015, 2019),
    "2020-2025" = c(2020, 2025)
  )
  
  # Zielw\u00f6rter f\u00fcr Frame-Shift-Analyse
  target_words <- c(
    # Feminizid-Begriffe
    "feminizid", "femizid", "frauenmord",
    # Gewalt-Begriffe (Grundformen nach Stemming)
    "hauslich_gewalt", "partnerschaft_gewalt", "bezieh_gewalt",
    # Familien-Framing
    "famili_drama", "famili_tragodi", "bezieh_tat",
    # T\u00e4ter-Begriffe
    "ehemann", "partner", "freund", "expartner",
    # Opfer-Begriffe
    "frau", "ehefrau", "partnerin", "opf"
  )

  target_map <- data.frame(
    original = target_words,
    normalized = normalize_terms_for_model(target_words),
    stringsAsFactors = FALSE
  )

  cat("=== WORD2VEC DIACHRONE ANALYSE ===\n")
  cat("Time windows:", length(time_windows), "\n")
  cat("Target words:", length(target_words), "\n")
  cat("Strategy: one model per window\n")
```

```{r word2vec-training}
stopifnot(exists("feminizid_corpus"))
  
  # F\u00fcr jedes Zeitfenster separates Word2Vec-Modell trainieren
  word2vec_models <- list()
  word2vec_embeddings <- list()
  
  for (window_name in names(time_windows)) {
    window_years <- time_windows[[window_name]]
    
    # Korpus f\u00fcr Zeitfenster filtern
    window_corpus <- corpus_subset(feminizid_corpus, 
                                  year >= window_years[1] & year <= window_years[2])
    
    # Mindestanzahl Dokumente pr\u00fcfen
    n_docs <- ndoc(window_corpus)
    if (n_docs < 100) {
      cat("WARN: Zeitfenster", window_name, "hat nur", n_docs, "Dokumente - \u00fcbersprungen\n")
      next
    }
    
    # Text f\u00fcr Word2Vec vorbereiten (bereits gestemmte Tokens verwenden)
    window_tokens <- tokens(window_corpus,
                           remove_punct = TRUE,
                           remove_symbols = TRUE,
                           remove_numbers = TRUE,
                           remove_url = TRUE) %>%
      tokens_tolower() %>%
      tokens_remove(all_stopwords, min_nchar = 3)

    if (USE_STEMMING) {
      window_tokens <- tokens_wordstem(window_tokens, language = "de")
    }
    
    # Multi-Word-Expressions anwenden
    for (pattern in simplified_patterns) {
      window_tokens <- tokens_compound(window_tokens, 
                                      pattern = phrase(paste(pattern, collapse = " ")),
                                      concatenator = "_")
    }
    
    # Zu Text f\u00fcr Word2Vec konvertieren
    window_text <- sapply(window_tokens, paste, collapse = " ")
    window_text <- window_text[nchar(window_text) > 50]  # Nur l\u00e4ngere Texte
    
    if (length(window_text) < 50) {
      cat("WARN: Zeitfenster", window_name, "hat zu wenig Text - \u00fcbersprungen\n")
      next
    }
    
    # Word2Vec-Modell trainieren
    cat("Training Word2Vec f\u00fcr", window_name, "(", n_docs, "Dokumente,", length(window_text), "Texte)...\n")
    
    w2v_model <- word2vec(x = window_text, 
                         type = "skip-gram",  # Skip-gram besser f\u00fcr seltene W\u00f6rter
                         dim = 100,           # 100 Dimensionen
                         iter = 20,           # 20 Iterationen
                         min_count = 5,       # Min. 5 Vorkommen
                         window = 5)          # Kontext-Fenster 5 W\u00f6rter
    
    word2vec_models[[window_name]] <- w2v_model
    
    # Embeddings for target words (normalized)
    target_terms_model <- unique(target_map$normalized)
    available_words <- intersect(target_terms_model, rownames(as.matrix(w2v_model)))
    if (length(available_words) > 0) {
      embeddings <- predict(w2v_model, available_words, type = "embedding")
      word2vec_embeddings[[window_name]] <- embeddings
      cat("Embeddings extracted for", length(available_words), "targets\n")
    } else {
      cat("WARN: No target words found in", window_name, "\n")
    }
  }
  
  cat("\n=== TRAINING ABGESCHLOSSEN ===\n")
  cat("Erfolgreiche Modelle:", length(word2vec_models), "\n")
  save_checkpoint(word2vec_models, "word2vec_models")
```

```{r word2vec-frame-shift}
if (length(word2vec_models) >= 2) {
  
  # Frame-Shift-Analyse: Was ist in der N\u00e4he von was?
  frame_shift_results <- list()
  
  for (i in seq_len(nrow(target_map))) {
    target_word <- target_map$original[i]
    target_norm <- target_map$normalized[i]
    
    # F\u00fcr jedes Zeitfenster die n\u00e4chsten Nachbarn finden
    neighbors_over_time <- list()
    
    for (window_name in names(word2vec_models)) {
      model <- word2vec_models[[window_name]]
      
      # Pr\u00fcfen ob Wort im Modell vorhanden
      if (target_norm %in% rownames(as.matrix(model))) {
        
        # Top 10 \u00e4hnlichste W\u00f6rter finden
        similar_words <- predict(model, target_norm, type = "nearest", top_n = 10)
        
        if (!is.null(similar_words) && length(similar_words) > 0) {
          neighbors_df <- data.frame(
            period = window_name,
            target = target_word,
            neighbor = similar_words[[1]]$term2,
            similarity = similar_words[[1]]$similarity,
            rank = similar_words[[1]]$rank,
            stringsAsFactors = FALSE
          )
          neighbors_over_time[[window_name]] <- neighbors_df
        }
      }
    }
    
    # Kombiniere alle Zeitfenster f\u00fcr dieses Zielwort
    if (length(neighbors_over_time) > 0) {
      combined_neighbors <- do.call(rbind, neighbors_over_time)
      frame_shift_results[[target_word]] <- combined_neighbors
    }
  }
  
  # Alle Ergebnisse kombinieren
  if (length(frame_shift_results) > 0) {
    all_frame_shifts <- do.call(rbind, frame_shift_results)
    save_checkpoint(all_frame_shifts, "word2vec_frame_shifts")
    
    cat("=== FRAME-SHIFT ANALYSE ===\n")
    cat("Analysierte W\u00f6rter:", length(frame_shift_results), "\n")
    cat("Gesamt Nachbar-Beziehungen:", nrow(all_frame_shifts), "\n")
    
    # Top 3 Nachbarn pro Zeitfenster fuer alle Zielwoerter
    for (term in names(frame_shift_results)) {
      cat("\n=== TARGET:", term, "===\n")
      term_shifts <- frame_shift_results[[term]]
      
      top_neighbors <- term_shifts %>%
        filter(rank <= 3) %>%
        select(period, neighbor, similarity) %>%
        arrange(period, -similarity)
      
      print(top_neighbors)
    }
  }
}
```

```{r word2vec-visualization}
if (exists("all_frame_shifts")) {
  
  # Visualisierung 1: Heatmap der Frame-Shifts
  library(ggplot2)
  library(dplyr)
  
  # Top-Nachbarn \u00fcber alle Zeitfenster
  top_shifts <- all_frame_shifts %>%
    filter(rank <= 5) %>%
    group_by(target, neighbor) %>%
    summarise(avg_similarity = mean(similarity), 
              periods_present = n(),
              .groups = 'drop') %>%
    filter(periods_present >= 2) %>%  # Mindestens in 2 Zeitfenstern
    arrange(target, -avg_similarity)
  
  # Heatmap f\u00fcr ausgew\u00e4hlte Zielw\u00f6rter
  selected_targets <- c("hauslich_gewalt", "feminizid", "famili_drama")
  available_targets <- intersect(selected_targets, unique(all_frame_shifts$target))
  
  if (length(available_targets) > 0) {
    heatmap_data <- all_frame_shifts %>%
      filter(target %in% available_targets, rank <= 5) %>%
      select(period, target, neighbor, similarity)
    
    p_heatmap <- ggplot(heatmap_data, aes(x = period, y = neighbor, fill = similarity)) +
      geom_tile(color = "white") +
      facet_wrap(~target, scales = "free_y") +
      scale_fill_gradient(low = "white", high = "darkred", name = "\u00c4hnlichkeit") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            axis.text.y = element_text(size = 8)) +
      labs(title = "Word2Vec Frame-Shifts \u00fcber Zeit",
           subtitle = "Welche W\u00f6rter sind in der N\u00e4he der Zielw\u00f6rter?",
           x = "Zeitfenster", y = "Nachbarw\u00f6rter")
    
    print(p_heatmap)
  }

  # Plot pro Target (Top-5 Nachbarn pro Zeitfenster)
  for (term in unique(all_frame_shifts$target)) {
    term_data <- all_frame_shifts %>%
      filter(target == term, rank <= 5) %>%
      select(period, neighbor, similarity)

    if (nrow(term_data) == 0) next

    p_term <- ggplot(term_data, aes(x = period, y = neighbor, fill = similarity)) +
      geom_tile(color = "white") +
      scale_fill_gradient(low = "white", high = "darkred", name = "Similarity") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            axis.text.y = element_text(size = 8)) +
      labs(title = paste("Word2Vec frame shifts:", term),
           x = "Time window", y = "Neighbors")

    print(p_term)
  }
  
  # Visualisierung 2: Stabilit\u00e4t vs. Wandel
  stability_analysis <- all_frame_shifts %>%
    group_by(target, neighbor) %>%
    summarise(
      periods_present = n(),
      avg_similarity = mean(similarity),
      similarity_sd = sd(similarity),
      .groups = 'drop'
    ) %>%
    mutate(
      stability = case_when(
        periods_present >= 4 & similarity_sd < 0.1 ~ "Stabil",
        periods_present >= 3 & similarity_sd < 0.2 ~ "M\u00e4\u00dfig stabil", 
        TRUE ~ "Wandelnd"
      )
    )
  
  cat("\n=== STABILIT\u00c4T DER FRAME-BEZIEHUNGEN ===\n")
  stability_summary <- stability_analysis %>%
    count(stability) %>%
    mutate(percentage = round(n / sum(n) * 100, 1))
  
  print(stability_summary)
}
```

## Embedding projector export

Export embeddings and metadata for the Embedding Projector visualization
([Google Research blog](https://research.google/blog/open-sourcing-the-embedding-projector-a-tool-for-visualizing-high-dimensional-data/)).

```{r word2vec-projector-export}
if (exists("word2vec_models")) {
  export_projector <- function(model, out_dir, terms, top_n = 10) {
    if (!dir.exists(out_dir)) dir.create(out_dir, recursive = TRUE)

    target_terms_model <- unique(target_map$normalized)
    in_vocab <- intersect(target_terms_model, rownames(as.matrix(model)))
    if (length(in_vocab) == 0) {
      message("No terms found in model vocabulary.")
      return(invisible(NULL))
    }

    neighbor_list <- lapply(in_vocab, function(term) {
      nn <- predict(model, term, type = "nearest", top_n = top_n)[[1]]$term2
      unique(c(term, nn))
    })
    vocab <- unique(unlist(neighbor_list))

    vectors <- as.matrix(model)[vocab, , drop = FALSE]
    meta <- data.frame(
      term = vocab,
      type = ifelse(vocab %in% in_vocab, "target", "neighbor"),
      stringsAsFactors = FALSE
    )

    write.table(vectors,
                file = file.path(out_dir, "vectors.tsv"),
                sep = "\t", row.names = FALSE, col.names = FALSE, quote = FALSE)
    write.table(meta,
                file = file.path(out_dir, "metadata.tsv"),
                sep = "\t", row.names = FALSE, col.names = TRUE, quote = FALSE)

    message("Saved projector files: ", out_dir)
  }

  # Pick a time window and export terms + nearest neighbors
  projector_window <- names(word2vec_models)[1]
  export_projector(
    model = word2vec_models[[projector_window]],
    out_dir = file.path("data", paste0("projector_", projector_window)),
    terms = target_words,
    top_n = 10
  )
}
```

## Local embedding visualization (R)

Visualize embeddings directly in R using UMAP (2D) and ggplot.

```{r word2vec-local-embedding}
if (exists("word2vec_models")) {
  if (!requireNamespace("umap", quietly = TRUE)) {
    message("Package 'umap' not installed. Install with: install.packages('umap')")
  } else {
    set.seed(123)

    # Pick a time window and build a local neighborhood for all target words
    viz_window <- names(word2vec_models)[1]
    model <- word2vec_models[[viz_window]]

    target_terms_model <- unique(target_map$normalized)
    in_vocab <- intersect(target_terms_model, rownames(as.matrix(model)))
    if (length(in_vocab) == 0) {
      message("No target words found in model vocabulary.")
    } else {
      neighbor_list <- lapply(in_vocab, function(term) {
        nn <- predict(model, term, type = "nearest", top_n = 10)[[1]]$term2
        unique(c(term, nn))
      })
      vocab <- unique(unlist(neighbor_list))

      vectors <- as.matrix(model)[vocab, , drop = FALSE]
      umap_coords <- umap::umap(vectors)$layout

      embedding_df <- data.frame(
        term = vocab,
        x = umap_coords[, 1],
        y = umap_coords[, 2],
        type = ifelse(vocab %in% in_vocab, "target", "neighbor"),
        stringsAsFactors = FALSE
      )

      if (requireNamespace("plotly", quietly = TRUE)) {
        p_umap <- plotly::plot_ly(
          embedding_df,
          x = ~x, y = ~y, color = ~type, text = ~term,
          type = "scatter", mode = "markers",
          marker = list(size = 7, opacity = 0.85)
        ) %>%
          plotly::layout(
            title = paste("UMAP Word2Vec embeddings:", viz_window),
            xaxis = list(title = "UMAP-1"),
            yaxis = list(title = "UMAP-2")
          )
        p_umap
      } else {
        message("Package 'plotly' not installed. Install with: install.packages('plotly')")
        p_umap <- ggplot(embedding_df, aes(x = x, y = y, color = type)) +
          geom_point(alpha = 0.8, size = 2) +
          theme_minimal() +
          labs(title = paste("UMAP Word2Vec embeddings:", viz_window),
               x = "UMAP-1", y = "UMAP-2", color = "Type")
        print(p_umap)
      }
    }
  }
}
```


