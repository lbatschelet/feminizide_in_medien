---
title: "Feminicide in Media - Data Collection"
output: html_notebook
encoding: UTF-8
---

# Setup

```{r setup, include=FALSE}
library(httr)
library(jsonlite)
library(yaml)
library(dotenv)
library(R6)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidytext)
library(readr)
library(tidyr)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(tm)
library(topicmodels)
library(wordcloud)
library(RColorBrewer)
library(umap)
library(plotly)
library(hunspell)

# API Wrapper laden
source("swissdox_api.R")
```

## Document Setup

Centralized API access, checkpoint utilities, and text configuration.

```{r document-setup}
# Load environment variables from local.env
if (file.exists("local.env")) {
  load_dot_env("local.env")
  message("\u2713 Environment variables loaded from local.env")
} else {
  message("WARN: local.env not found - using system environment")
}

# Initialize API client with credentials
client <- SwissdoxAPI$new(
  api_key = Sys.getenv("SWISSDOX_API_KEY"),
  api_secret = Sys.getenv("SWISSDOX_API_SECRET")
)
# Checkpoint helpers (load/save latest timestamped files)
find_latest <- function(pattern, dir = "data") {
  files <- list.files(dir, pattern = pattern, full.names = TRUE)
  if (length(files) == 0) return(NULL)
  files[order(file.mtime(files), decreasing = TRUE)][1]
}

latest_checkpoint <- function(prefix, dir = "data") {
  pattern <- paste0("^", prefix, "_\\d{8}_\\d{6}\\.rds$")
  find_latest(pattern, dir)
}

save_checkpoint <- function(obj, prefix, dir = "data", compress = "xz") {
  file <- file.path(dir, paste0(prefix, "_", format(Sys.time(), "%Y%m%d_%H%M%S"), ".rds"))
  saveRDS(obj, file, compress = compress)
  message("Saved checkpoint: ", basename(file))
  file
}

load_checkpoint <- function(prefix, var_name) {
  file <- latest_checkpoint(prefix)
  if (is.null(file)) {
    message("No checkpoint found for: ", prefix)
    return(NULL)
  }
  assign(var_name, readRDS(file), envir = .GlobalEnv)
  message("Loaded checkpoint: ", basename(file))
  file
}
# Text analysis defaults
SAVE_CORPUS <- FALSE
USE_STEMMING <- TRUE
TRAIN_FULL_MODEL <- TRUE
FULL_MODEL_LABEL <- "all_2000-2025"

# Multi-word patterns after stemming (used in preprocessing + word2vec)
simplified_patterns <- list(
  c("hauslich", "gewalt"),
  c("partnerschaft", "gewalt"),
  c("partner", "gewalt"),
  c("bezieh", "gewalt"),
  c("sexuell", "gewalt"),
  c("körperl", "gewalt"),
  c("psychisch", "gewalt"),
  c("töt", "frau"),
  c("mord", "frau"),
  c("gewalt", "frau"),
  c("famili", "drama"),
  c("famili", "tragödi"),
  c("bezieh", "tat"),
  c("eh", "streit"),
  c("opf", "schutz"),
  c("gewalt", "schutz")
)

# Normalize terms to match Word2Vec vocabulary (lowercase, optional stemming)
normalize_terms_for_model <- function(terms) {
  norm <- stringr::str_to_lower(terms)
  toks <- quanteda::tokens(norm, remove_punct = TRUE)
  if (USE_STEMMING) {
    toks <- quanteda::tokens_wordstem(toks, language = "de")
  }
  vapply(toks, function(x) paste(x, collapse = "_"), character(1))
}

```

## Keyword Definitions

These terms are the basis for the queries. We define them centrally to keep
usage consistent across analysis and API calls.

```{r keyword-definitions}

feminizid_keywords <- c(
  "Feminizid", "Femizid", "Frauenmord",
  "Familiendrama", "Familientragödie", "Beziehungstat", "Ehestreit",
  "häusliche Gewalt", "Partnerschaftsgewalt", "Gewalt gegen Frauen",
  "Tötung von Frau", "Mord an Frau", "Partnergewalt", "Beziehungsgewalt"
)

```

## Data collection

Build and run the Swissdox query. Use the checkpoint load block to resume
without re-running the API call.

We start with a broad exploratory query using the original keyword list (OR).

```{r checkpoint-load-raw}
# Checkpoint load (raw data)
load_checkpoint("feminizid_raw_data", "media_data")
```

```{r feminizid-data-collection}
# Full data collection with API wrapper
collection_result <- execute_swissdox_workflow(
  client = client,
  query = create_swissdox_query(
    content_terms = feminizid_keywords,
    date_from = "2000-01-01",
    date_to = format(Sys.Date(), "%Y-%m-%d"),
    languages = "de",
    max_results = 1000000L,
    columns = c("id", "pubtime", "medium_code", "medium_name", "rubric",
               "regional", "doctype", "doctype_description", "language", 
               "char_count", "dateline", "head", "subhead", 
               "article_link", "content_id", "content")
  ),
  name = paste("Feminizid_Collection", format(Sys.time(), "%Y%m%d_%H%M%S")),
  comment = "Comprehensive feminicide media analysis dataset",
  return_data = TRUE,
  output_dir = "data"
)

# Store results and save checkpoint
if (!is.null(collection_result$data)) {
  media_data <- collection_result$data
  save_checkpoint(media_data, "feminizid_raw_data")
  message("\u2713 ", collection_result$message)
} else {
  message("Error while loading data")
}
```

## Data preparation

Load raw data (from checkpoint or TSV), clean fields, and prepare time
variables used in analysis.

```{r data-preparation}
# Load data (checkpoint first, TSV fallback)
raw_file <- load_checkpoint("feminizid_raw_data", "media_data")
if (is.null(raw_file)) {
  tsv_files <- list.files("data", pattern = "\\.tsv(\\.xz)?$", full.names = TRUE)
  if (length(tsv_files) == 0) {
    message("WARN: No data files found - run the API call")
  } else {
    message("No .rds file found, loading .tsv files...")
    media_data <- bind_rows(lapply(tsv_files, function(file) {
      message("Loading: ", basename(file))
      read_tsv(file, locale = locale(encoding = "UTF-8"))
    })) %>% distinct(id, .keep_all = TRUE)
    message("\u2713 Combined ", length(tsv_files), " files: ", nrow(media_data), " articles")
    save_checkpoint(media_data, "feminizid_raw_data")
  }
}
```

```{r data-cleaning}

# Prepare data (if not already done)
# Convert dates and create time variables
media_data$pubtime <- as.POSIXct(media_data$pubtime)
media_data$pub_date <- as.Date(media_data$pubtime)
media_data$year <- format(media_data$pub_date, "%Y")
media_data$month <- format(media_data$pub_date, "%m")
media_data$month_year <- format(media_data$pub_date, "%Y-%m")

# Keep German-only articles (drop French)
media_data <- media_data %>%
  filter(language == "de") %>%
  filter(!is.na(content), content != "", nchar(content) > 50)

message("German articles after filtering: ", nrow(media_data))
message("Date range: ", min(media_data$pub_date, na.rm = TRUE), " to ", max(media_data$pub_date, na.rm = TRUE))
message("Media outlets: ", length(unique(media_data$medium_name)))

```

```{r overview-analysis}

# 1. Articles per year
yearly_counts <- media_data %>%
  count(year) %>%
  filter(!is.na(year), year >= 2000, year <= 2025)

p1 <- ggplot(yearly_counts, aes(x = as.numeric(year), y = n)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Feminicide coverage over time", 
       subtitle = "German media 2000-2025",
       x = "Year", y = "Article count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 2. Top outlets
top_media <- media_data %>%
  count(medium_name, sort = TRUE) %>%
  head(15)

p2 <- ggplot(top_media, aes(x = reorder(medium_name, n), y = n)) +
  geom_col(fill = "coral", alpha = 0.7) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 15 outlets", x = "Outlet", y = "Article count")

# 3. Monthly trend (last 5 years)
recent_trend <- media_data %>%
  filter(year >= 2020) %>%
  count(month_year) %>%
  arrange(month_year)

p3 <- ggplot(recent_trend, aes(x = month_year, y = n)) +
  geom_line(group = 1, color = "darkgreen", size = 1) +
  geom_point(color = "darkgreen", size = 1.5) +
  theme_minimal() +
  labs(title = "Monthly trend 2020-2026", x = "Month", y = "Article count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))

print(p1)
print(p2) 
print(p3)

# Summary
cat("\n=== DATA OVERVIEW ===\n")
cat("German articles:", nrow(media_data), "\n")
cat("Date range:", min(media_data$pub_date, na.rm = TRUE), "to", max(media_data$pub_date, na.rm = TRUE), "\n")
cat("Media outlets:", length(unique(media_data$medium_name)), "\n")
cat("Average per year:", round(nrow(media_data) / length(unique(media_data$year))), "\n")
```

```{r keyword-analysis}

# Keyword frequency in headlines (central definition)
keyword_counts <- data.frame(keyword = character(), count = numeric())

for (kw in feminizid_keywords) {
  count <- sum(str_detect(media_data$head, regex(kw, ignore_case = TRUE)), na.rm = TRUE)
  keyword_counts <- rbind(keyword_counts, data.frame(keyword = kw, count = count))
}

keyword_counts <- keyword_counts %>%
  filter(count > 0) %>%
  arrange(desc(count))

p4 <- ggplot(keyword_counts, aes(x = reorder(keyword, count), y = count)) +
  geom_col(fill = "darkred", alpha = 0.7) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Keyword frequency in headlines", 
       x = "Keyword", y = "Article count")

print(p4)

# Print top keywords
cat("\n=== KEYWORD FREQUENCY ===\n")
print(keyword_counts)
```

```{r time-trends}

# Keyword time trends (central definition)
keyword_timeline <- data.frame()

for (kw in feminizid_keywords) {
  yearly_kw <- media_data %>%
    filter(!is.na(year), !is.na(head)) %>%
    mutate(has_keyword = str_detect(head, regex(kw, ignore_case = TRUE))) %>%
    group_by(year) %>%
    summarise(
      total_articles = n(),
      keyword_articles = sum(has_keyword, na.rm = TRUE),
      percentage = (keyword_articles / total_articles) * 100,
      .groups = 'drop'
    ) %>%
    mutate(keyword = kw)
  
  keyword_timeline <- bind_rows(keyword_timeline, yearly_kw)
}

# Keep relevant keywords and years
keyword_timeline <- keyword_timeline %>%
  filter(year >= 2000, year <= 2025) %>%
  group_by(keyword) %>%
  filter(sum(keyword_articles) > 10) %>%  # At least 10 articles overall
  ungroup()

# Absolute counts over time
p5 <- ggplot(keyword_timeline, aes(x = as.numeric(year), y = keyword_articles, color = keyword)) +
  geom_line(size = 1, alpha = 0.8) +
  geom_point(size = 1.5, alpha = 0.7) +
  theme_minimal() +
  labs(title = "Time trends: keyword usage in headlines",
       subtitle = "Absolute article counts per year",
       x = "Year", y = "Article count", color = "Keyword") +
  theme(legend.position = "bottom", legend.text = element_text(size = 8)) +
  guides(color = guide_legend(ncol = 2))

# Heatmap: keywords x years
keyword_matrix <- keyword_timeline %>%
  select(year, keyword, keyword_articles) %>%
  pivot_wider(names_from = keyword, values_from = keyword_articles, values_fill = 0) %>%
  pivot_longer(-year, names_to = "keyword", values_to = "count")

p6 <- ggplot(keyword_matrix, aes(x = as.numeric(year), y = keyword, fill = count)) +
  geom_tile(color = "white", size = 0.1) +
  scale_fill_gradient(low = "white", high = "darkred", name = "Articles") +
  theme_minimal() +
  labs(title = "Keyword intensity over time", x = "Year", y = "Keyword") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(size = 9))

print(p5)
print(p6)

# Key trends
cat("\n=== TIME TREND ANALYSIS ===\n")
trend_summary <- keyword_timeline %>%
  group_by(keyword) %>%
  summarise(
    total_articles = sum(keyword_articles),
    peak_year = year[which.max(keyword_articles)],
    peak_count = max(keyword_articles),
    .groups = 'drop'
  ) %>%
  arrange(desc(total_articles))

print(trend_summary)
```

## Corpus and preprocessing

Build a quanteda corpus and a trimmed DFM from the cleaned articles. Use the
checkpoint load block to resume without reprocessing.

```{r checkpoint-load-corpus}
# Checkpoint load (corpus/dfm)
load_checkpoint("feminizid_corpus", "feminizid_corpus")
load_checkpoint("feminizid_dfm", "feminizid_dfm_trimmed")
```

```{r corpus-setup}

corpus_data <- media_data %>%
  distinct(id, .keep_all = TRUE) %>%
  filter(!is.na(content), content != "", nchar(content) > 100) %>%
  mutate(unique_id = paste0("doc_", row_number()))

message("Articles for corpus: ", nrow(corpus_data))

corpus_data$clean_content <- corpus_data$content %>%
  str_remove_all("<[^>]+>") %>%
  str_remove_all("\\n|\\t|\\r") %>%
  str_remove_all("https?://\\S+") %>%
  str_squish()

corpus_data <- corpus_data %>%
  filter(!is.na(clean_content), clean_content != "", nchar(clean_content) > 50)

feminizid_corpus <- corpus(corpus_data,
                            text_field = "clean_content",
                            docid_field = "unique_id")

docvars(feminizid_corpus, "original_id") <- corpus_data$id
docvars(feminizid_corpus, "medium") <- corpus_data$medium_name
docvars(feminizid_corpus, "date") <- corpus_data$pub_date
docvars(feminizid_corpus, "year") <- corpus_data$year
docvars(feminizid_corpus, "headline") <- corpus_data$head
docvars(feminizid_corpus, "char_count") <- as.numeric(corpus_data$char_count)

cat("=== QUANTEDA CORPUS ===\n")
cat("Documents:", ndoc(feminizid_corpus), "\n")
cat("Tokens:", sum(ntoken(feminizid_corpus)), "\n")
cat("Range:", min(corpus_data$pub_date, na.rm = TRUE), "to", max(corpus_data$pub_date, na.rm = TRUE), "\n")

print(summary(feminizid_corpus, n = 3))

save_checkpoint(feminizid_corpus, "feminizid_corpus")
```

```{r text-preprocessing}

# Load German stopwords
german_stopwords <- readLines("german_stopwords_full.txt", encoding = "UTF-8")
german_stopwords <- german_stopwords[german_stopwords != ""]

# Media-specific stopwords
media_stopwords <- c("uhr", "jahr", "jahre", "jahren", "prozent", "franken", "chf",
                    "artikel", "bericht", "zeitung", "medium", "redaktion", "autor",
                    "quelle", "foto", "bild", "video", "online", "www", "http",
                    "montag", "dienstag", "mittwoch", "donnerstag", "freitag",
                    "samstag", "sonntag", "januar", "februar", "märz", "april",
                    "mai", "juni", "juli", "august", "september", "oktober",
                    "november", "dezember", "schweiz", "schweizer", "schweizerin")

all_stopwords <- c(german_stopwords, media_stopwords)


# Tokenisierung und Preprocessing
feminizid_tokens <- tokens(feminizid_corpus,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          remove_numbers = TRUE,
                          remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(all_stopwords, min_nchar = 3)

# Stemming (optional)
if (USE_STEMMING) {
  feminizid_tokens <- tokens_wordstem(feminizid_tokens, language = "de")
}

# Then multi-word expressions (now with base forms)
# Simplified patterns - base forms are sufficient
# Uses simplified_patterns defined in text-config

# Compound multi-word expressions into single tokens
for (pattern in simplified_patterns) {
  feminizid_tokens <- tokens_compound(feminizid_tokens, 
                                      pattern = phrase(paste(pattern, collapse = " ")),
                                      concatenator = "_")
}

cat("=== TEXT PREPROCESSING STEPS ===\n")
cat("1. Stemming first: reduces inflectional variants\n")
cat("   häusliche/häuslicher \u2192 häusl, tötung \u2192 töt, frauen \u2192 frau\n")
cat("2. Then multi-word expressions:", length(simplified_patterns), "word pairs\n")
cat("   häusl + gewalt \u2192 häusl_gewalt (captures all inflections)\n")
cat("3. Benefit: one pattern captures multiple variants\n")

# Document-Feature-Matrix
feminizid_dfm <- dfm(feminizid_tokens)

# Trimming: consistent parameters (both as proportions)
# Based on quanteda documentation: https://quanteda.io/reference/dfm_trim.html
feminizid_dfm_trimmed <- dfm_trim(feminizid_dfm, 
                                  min_docfreq = 0.0003,  # Min 0.03% (~11 docs at 35k)
                                  max_docfreq = 0.90,    # Max 90% of documents
                                  docfreq_type = "prop")  # Both as proportions

cat("=== TEXT PREPROCESSING ===\n")
cat("Documents:", ndoc(feminizid_dfm), "\n")
cat("Original Features:", nfeat(feminizid_dfm), "\n")
cat("After trimming:", nfeat(feminizid_dfm_trimmed), "\n")
cat("Sparsity:", round(sparsity(feminizid_dfm_trimmed), 3), "\n")

# Most frequent words
if (nfeat(feminizid_dfm_trimmed) > 0) {
  top_features <- topfeatures(feminizid_dfm_trimmed, 25)
  cat("\n=== MOST FREQUENT WORDS ===\n")
  print(top_features)
} else {
  cat("\nWARN: No features after trimming - adjust parameters\n")
}
```

```{r checkpoint-save-dfm}
# Checkpoint save (DFM and optional corpus)
if (nfeat(feminizid_dfm_trimmed) > 0) {
  save_checkpoint(feminizid_dfm_trimmed, "feminizid_dfm")
}
if (SAVE_CORPUS) {
  save_checkpoint(feminizid_corpus, "feminizid_corpus")
}
```

```{r visualization}
# Use quanteda.textplots if available, otherwise fallback to wordcloud
if ("textplot_wordcloud" %in% ls("package:quanteda.textplots")) {
  textplot_wordcloud(feminizid_dfm_trimmed, 
                    min_count = 30,
                    max_words = 100,
                    color = brewer.pal(8, "Dark2"))
} else {
  # Alternative mit wordcloud package
  freq_data <- topfeatures(feminizid_dfm_trimmed, 100)
  wordcloud(names(freq_data), freq_data, 
           min.freq = 30, max.words = 100,
           colors = brewer.pal(8, "Dark2"),
           random.order = FALSE)
}

# Feature frequency over time (normalized)
# Top 10 features by year
top_terms <- names(topfeatures(feminizid_dfm_trimmed, 10))
  
# Frequency of these terms over years
yearly_terms <- dfm_group(feminizid_dfm_trimmed, groups = docvars(feminizid_dfm_trimmed, "year"))

# Articles per year for normalization
articles_per_year <- feminizid_corpus %>%
  docvars() %>%
  mutate(year = as.numeric(year)) %>%  # Convert year to numeric
  count(year, name = "total_articles") %>%
  filter(year >= 2000, year <= 2025)

# Correct conversion for quanteda 3.0+ - without doc_id column
term_data <- convert(yearly_terms[, top_terms], to = "data.frame") %>%
  select(-doc_id) %>%  # Remove doc_id column
  mutate(year = as.numeric(rownames(yearly_terms)))

# Absolute frequencies
term_timeline_abs <- term_data %>%
  pivot_longer(-year, names_to = "term", values_to = "frequency") %>%
  filter(year >= 2000, year <= 2025)

# Normalized frequencies (per 1000 articles)
term_timeline_norm <- term_timeline_abs %>%
  left_join(articles_per_year, by = "year") %>%
  mutate(frequency_per_1000 = (frequency / total_articles) * 1000) %>%
  filter(!is.na(total_articles), total_articles > 10)  # Only years with >10 articles

# Plot 1: Absolute frequencies
p7a <- ggplot(term_timeline_abs, aes(x = year, y = frequency, color = term)) +
  geom_line(size = 1, alpha = 0.8) +
  theme_minimal() +
  labs(title = "Most frequent terms over time (absolute)", 
        x = "Year", y = "Absolute frequency", color = "Term") +
  theme(legend.position = "bottom")

# Plot 2: Normalized frequencies (per 1000 articles)
p7b <- ggplot(term_timeline_norm, aes(x = year, y = frequency_per_1000, color = term)) +
  geom_line(size = 1, alpha = 0.8) +
  theme_minimal() +
  labs(title = "Most frequent terms over time (normalized per 1000 articles)", 
        x = "Year", y = "Frequency per 1000 articles", color = "Term") +
  theme(legend.position = "bottom")

# Plot 3: Article distribution over time (sanity check)
p7c <- ggplot(articles_per_year, aes(x = year, y = total_articles)) +
  geom_line(size = 1, color = "darkblue") +
  geom_point(size = 2, color = "darkblue") +
  theme_minimal() +
  labs(title = "Article distribution over time", 
        x = "Year", y = "Article count") +
  scale_y_continuous(labels = scales::comma)

print(p7a)
print(p7b) 
print(p7c)

# Normalization summary
cat("\n=== NORMALIZATION SUMMARY ===\n")
summary_stats <- articles_per_year %>%
  summarise(
    min_articles = min(total_articles),
    max_articles = max(total_articles),
    mean_articles = round(mean(total_articles)),
    median_articles = median(total_articles),
    ratio_max_min = round(max(total_articles) / min(total_articles), 1)
  )

cat("Min articles per year:", summary_stats$min_articles, "\n")
cat("Max articles per year:", summary_stats$max_articles, "\n")
cat("Durchschnitt:", summary_stats$mean_articles, "\n")
cat("Median:", summary_stats$median_articles, "\n")
cat("Max/min ratio:", summary_stats$ratio_max_min, "x\n")

if (summary_stats$ratio_max_min > 5) {
  cat("WARN: Strong distortion detected - normalized view recommended\n")
}

```

## Word2Vec frame shift (diachronic)

Train one Word2Vec model per time window and compare nearest neighbors
across periods. Use the checkpoint load block to resume without retraining.

```{r checkpoint-load-word2vec}
# Checkpoint load (Word2Vec)
load_checkpoint("word2vec_models", "word2vec_models")
load_checkpoint("word2vec_frame_shifts", "all_frame_shifts")
```

```{r word2vec-setup}
library(word2vec)

# Time windows (5-year bins)
time_windows <- list(
  "2000-2004" = c(2000, 2004),
  "2005-2009" = c(2005, 2009), 
  "2010-2014" = c(2010, 2014),
  "2015-2019" = c(2015, 2019),
  "2020-2025" = c(2020, 2025)
)

# Target terms for frame-shift analysis
target_words <- c(
  # Feminicide terms
  "feminizid", "femizid", "frauenmord",
  # Violence terms (base forms after stemming)
  "hauslich_gewalt", "partnerschaft_gewalt", "bezieh_gewalt",
  # Family framing
  "famili_drama", "famili_tragodi", "bezieh_tat",
  # Perpetrator terms
  "ehemann", "partner", "freund", "expartner",
  # Victim terms
  "frau", "ehefrau", "partnerin", "opf"
)

target_map <- data.frame(
  original = target_words,
  normalized = normalize_terms_for_model(target_words),
  stringsAsFactors = FALSE
)

cat("=== WORD2VEC DIACHRONIC ANALYSIS ===\n")
cat("Time windows:", length(time_windows), "\n")
cat("Target words:", length(target_words), "\n")
cat("Strategy: one model per window\n")
if (TRAIN_FULL_MODEL) {
  cat("Full-period model:", FULL_MODEL_LABEL, "\n")
}
```

```{r word2vec-training}

# Train models for each window + optional full-period model
word2vec_models <- list()
word2vec_embeddings <- list()

# Optional progress bar (per window)
use_progress <- requireNamespace("progress", quietly = TRUE)
if (!use_progress) {
  message("Tip: install.packages('progress') for a progress bar.")
}

build_w2v_text <- function(corpus_obj) {
  tokens_obj <- tokens(corpus_obj,
                        remove_punct = TRUE,
                        remove_symbols = TRUE,
                        remove_numbers = TRUE,
                        remove_url = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(all_stopwords, min_nchar = 3)
  if (USE_STEMMING) {
    tokens_obj <- tokens_wordstem(tokens_obj, language = "de")
  }
  for (pattern in simplified_patterns) {
    tokens_obj <- tokens_compound(tokens_obj,
                                  pattern = phrase(paste(pattern, collapse = " ")),
                                  concatenator = "_")
  }
  txt <- sapply(tokens_obj, paste, collapse = " ")
  txt[nchar(txt) > 50]
}

train_w2v <- function(label, corpus_obj) {
  n_docs <- ndoc(corpus_obj)
  txt <- build_w2v_text(corpus_obj)
  if (n_docs < 100 || length(txt) < 50) {
    cat("WARN: Skip", label, "- too little data\n")
    return(NULL)
  }
  cat("Training Word2Vec for", label, "(", n_docs, "docs,", length(txt), "texts)...\n")
  word2vec(x = txt,
            type = "skip-gram",
            dim = 100,
            iter = 20,
            min_count = 5,
            window = 5)
}

pb <- if (use_progress) progress::progress_bar$new(
  format = "Training :current/:total [:bar] :percent :eta",
  total = length(time_windows),
  clear = FALSE,
  width = 60
) else NULL

for (window_name in names(time_windows)) {
  if (use_progress) pb$tick()
  window_years <- time_windows[[window_name]]
  window_corpus <- corpus_subset(
    feminizid_corpus,
    year >= window_years[1] & year <= window_years[2]
  )
  model <- train_w2v(window_name, window_corpus)
  if (is.null(model)) next
  word2vec_models[[window_name]] <- model

  target_terms_model <- unique(target_map$normalized)
  available_words <- intersect(target_terms_model, rownames(as.matrix(model)))
  if (length(available_words) > 0) {
    word2vec_embeddings[[window_name]] <- predict(model, available_words, type = "embedding")
  } else {
    cat("WARN: No target words found in", window_name, "\n")
  }
}

if (TRAIN_FULL_MODEL) {
  full_model <- train_w2v(FULL_MODEL_LABEL, feminizid_corpus)
  if (!is.null(full_model)) {
    word2vec_models[[FULL_MODEL_LABEL]] <- full_model
  }
}

cat("\n=== TRAINING DONE ===\n")
cat("Models:", length(word2vec_models), "\n")
# Always save a checkpoint after training (expensive step)
save_checkpoint(word2vec_models, "word2vec_models")
```

```{r word2vec-frame-shift}
window_models <- intersect(names(word2vec_models), names(time_windows))
if (length(window_models) >= 2) {
  
  # Frame-shift analysis: what is near what?
  frame_shift_results <- list()
  
  for (i in seq_len(nrow(target_map))) {
    target_word <- target_map$original[i]
    target_norm <- target_map$normalized[i]
    
    # Find nearest neighbors for each time window
    neighbors_over_time <- list()
    
    for (window_name in window_models) {
      model <- word2vec_models[[window_name]]
      
      # Check whether term exists in model vocabulary
      if (target_norm %in% rownames(as.matrix(model))) {
        
        # Find top 10 nearest words
        similar_words <- predict(model, target_norm, type = "nearest", top_n = 10)
        
        if (!is.null(similar_words) && length(similar_words) > 0) {
          neighbors_df <- data.frame(
            period = window_name,
            target = target_word,
            neighbor = similar_words[[1]]$term2,
            similarity = similar_words[[1]]$similarity,
            rank = similar_words[[1]]$rank,
            stringsAsFactors = FALSE
          )
          neighbors_over_time[[window_name]] <- neighbors_df
        }
      }
    }
    
    # Combine all time windows for this target term
    if (length(neighbors_over_time) > 0) {
      combined_neighbors <- do.call(rbind, neighbors_over_time)
      frame_shift_results[[target_word]] <- combined_neighbors
    }
  }
  
  # Alle Ergebnisse kombinieren
  if (length(frame_shift_results) > 0) {
    all_frame_shifts <- do.call(rbind, frame_shift_results)
    save_checkpoint(all_frame_shifts, "word2vec_frame_shifts")
    
    cat("=== FRAME SHIFT ANALYSIS ===\n")
    cat("Analyzed terms:", length(frame_shift_results), "\n")
    cat("Total neighbor relations:", nrow(all_frame_shifts), "\n")
    
    # Top 3 neighbors per time window for all target terms
    for (term in names(frame_shift_results)) {
      cat("\n=== TARGET:", term, "===\n")
      term_shifts <- frame_shift_results[[term]]
      
      top_neighbors <- term_shifts %>%
        filter(rank <= 3) %>%
        select(period, neighbor, similarity) %>%
        arrange(period, -similarity)
      
      print(top_neighbors)
    }
  }
}
```

```{r word2vec-visualization}

# Visualisierung 1: Heatmap der Frame-Shifts
library(ggplot2)
library(dplyr)

# Top neighbors across all time windows
top_shifts <- all_frame_shifts %>%
  filter(rank <= 5) %>%
  group_by(target, neighbor) %>%
  summarise(avg_similarity = mean(similarity), 
            periods_present = n(),
            .groups = 'drop') %>%
  filter(periods_present >= 2) %>%  # At least in 2 time windows
  arrange(target, -avg_similarity)

# Heatmap for selected target terms
selected_targets <- c("hauslich_gewalt", "feminizid", "famili_drama")
available_targets <- intersect(selected_targets, unique(all_frame_shifts$target))

if (length(available_targets) > 0) {
  heatmap_data <- all_frame_shifts %>%
    filter(target %in% available_targets, rank <= 5) %>%
    select(period, target, neighbor, similarity)
  
  p_heatmap <- ggplot(heatmap_data, aes(x = period, y = neighbor, fill = similarity)) +
    geom_tile(color = "white") +
    facet_wrap(~target, scales = "free_y") +
    scale_fill_gradient(low = "white", high = "darkred", name = "Ähnlichkeit") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(size = 8)) +
    labs(title = "Word2Vec frame shifts over time",
       subtitle = "Which words are close to the target terms?",
       x = "Time window", y = "Neighbor terms")
  
  print(p_heatmap)
}

# Plot per target (top 5 neighbors per time window)
for (term in unique(all_frame_shifts$target)) {
  term_data <- all_frame_shifts %>%
    filter(target == term, rank <= 5) %>%
    select(period, neighbor, similarity)

  if (nrow(term_data) == 0) next

  p_term <- ggplot(term_data, aes(x = period, y = neighbor, fill = similarity)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "white", high = "darkred", name = "Similarity") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(size = 8)) +
    labs(title = paste("Word2Vec frame shifts:", term),
         x = "Time window", y = "Neighbors")

  print(p_term)
}

# Visualization 2: stability vs. change
stability_analysis <- all_frame_shifts %>%
  group_by(target, neighbor) %>%
  summarise(
    periods_present = n(),
    avg_similarity = mean(similarity),
    similarity_sd = sd(similarity),
    .groups = 'drop'
  ) %>%
  mutate(
      stability = case_when(
        periods_present >= 4 & similarity_sd < 0.1 ~ "Stable",
        periods_present >= 3 & similarity_sd < 0.2 ~ "Moderately stable", 
        TRUE ~ "Changing"
      )
  )

cat("\n=== STABILITY OF FRAME RELATIONS ===\n")
stability_summary <- stability_analysis %>%
  count(stability) %>%
  mutate(percentage = round(n / sum(n) * 100, 1))

print(stability_summary)
```

## Local embedding visualization

We reduce the 100-dimensional Word2Vec space to 3D with UMAP to inspect local
semantic neighborhoods. UMAP builds a nearest-neighbor graph in the original
space and optimizes a low-dimensional layout that preserves those local
neighborhoods. The plotted axes are not interpretable dimensions; they are
coordinates of a layout that preserves local structure.

Interpretation:
- Points close in 3D usually appear in similar contexts in the corpus.
- Tight clusters suggest a coherent framing or shared discourse context.
- Isolated points can indicate rare usage or distinct framing.

Caveats:
- Distances are mainly meaningful locally (nearest neighbors).
- Global positions and axis directions should not be over-interpreted.

```{r word2vec-local-embedding}
set.seed(123)

# Prefer full-period model if available, otherwise latest window
if (FULL_MODEL_LABEL %in% names(word2vec_models)) {
  viz_window <- FULL_MODEL_LABEL
} else {
  available_windows <- intersect(names(word2vec_models), names(time_windows))
  if (length(available_windows) > 0) {
    viz_window <- tail(available_windows, 1)
  } else {
    viz_window <- tail(names(word2vec_models), 1)
  }
}
message("Using window: ", viz_window)
model <- word2vec_models[[viz_window]]

target_terms_model <- unique(target_map$normalized)
in_vocab <- intersect(target_terms_model, rownames(as.matrix(model)))
if (length(in_vocab) == 0) {
  message("No target words found in model vocabulary.")
} else {
  neighbor_list <- lapply(in_vocab, function(term) {
    nn <- predict(model, term, type = "nearest", top_n = 10)[[1]]$term2
    unique(c(term, nn))
  })
  vocab <- unique(unlist(neighbor_list))

  vectors <- as.matrix(model)[vocab, , drop = FALSE]
  umap_coords <- umap::umap(vectors, n_components = 3)$layout

  embedding_df <- data.frame(
    term = vocab,
    x = umap_coords[, 1],
    y = umap_coords[, 2],
    z = umap_coords[, 3],
    type = ifelse(vocab %in% in_vocab, "target", "neighbor"),
    stringsAsFactors = FALSE
  )

  p_umap <- plotly::plot_ly(
    embedding_df,
    x = ~x, y = ~y, z = ~z, color = ~type, text = ~term,
    type = "scatter3d", mode = "markers",
    marker = list(size = 7, opacity = 0.85)
  ) %>%
    plotly::layout(
      title = paste("Word2Vec embeddings (UMAP 3D):", viz_window),
      scene = list(
        xaxis = list(title = "UMAP dim 1"),
        yaxis = list(title = "UMAP dim 2"),
        zaxis = list(title = "UMAP dim 3")
      )
    )
  p_umap
}
```


## Iterative query expansion

We use the Word2Vec neighborhoods to suggest additional search terms that
may reflect earlier or alternative framing. This does not replace manual
review; it provides a ranked candidate list for curation and then a second
API query.

```{r query-expansion}
# Seeds in the model vocabulary (normalized form)
seed_terms <- unique(target_map$normalized)

collect_neighbors <- function(model, seeds, top_n = 30) {
  seeds_in_vocab <- intersect(seeds, rownames(as.matrix(model)))
  res <- lapply(seeds_in_vocab, function(seed) {
    nn <- predict(model, seed, type = "nearest", top_n = top_n)[[1]]
    data.frame(
      seed = seed,
      term = nn$term2,
      similarity = nn$similarity,
      rank = nn$rank,
      stringsAsFactors = FALSE
    )
  })
  dplyr::bind_rows(res)
}

neighbors_all <- dplyr::bind_rows(lapply(names(word2vec_models), function(w) {
  model <- word2vec_models[[w]]
  df <- collect_neighbors(model, seed_terms, top_n = 30)
  df$window <- w
  df
}))

candidate_summary <- neighbors_all %>%
  filter(!(term %in% seed_terms)) %>%
  group_by(term) %>%
  summarise(
    mean_similarity = mean(similarity),
    max_similarity = max(similarity),
    seed_hits = n_distinct(seed),
    window_hits = n_distinct(window),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_similarity))

# Heuristic: candidates that are close to multiple seeds and appear across windows
suggested_terms <- candidate_summary %>%
  filter(mean_similarity >= 0.35, seed_hits >= 2, window_hits >= 2) %>%
  pull(term)

# Basic cleanup: keep alpha tokens, remove very short / rare items
term_freq <- textstat_frequency(feminizid_dfm_trimmed, n = Inf) %>%
  select(feature, frequency)

suggested_terms_clean <- stringr::str_to_lower(suggested_terms)
suggested_terms_clean <- suggested_terms_clean[
  stringr::str_detect(suggested_terms_clean, "^[[:alpha:]_]+$")
]
suggested_terms_clean <- unique(suggested_terms_clean)
suggested_terms_clean <- Filter(function(x) nchar(x) >= 4, suggested_terms_clean)

suggested_terms_clean <- term_freq %>%
  filter(feature %in% suggested_terms_clean, frequency >= 5) %>%
  arrange(desc(frequency)) %>%
  pull(feature)

# Map stems back to likely surface forms (approximation)
MAP_DOCS_LIMIT <- 10000
corpus_for_map <- if (ndoc(feminizid_corpus) > MAP_DOCS_LIMIT) {
  corpus_sample(feminizid_corpus, size = MAP_DOCS_LIMIT)
} else {
  feminizid_corpus
}

tokens_raw <- tokens(corpus_for_map,
                     remove_punct = TRUE,
                     remove_symbols = TRUE,
                     remove_numbers = TRUE,
                     remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(all_stopwords, min_nchar = 3)

tokens_stem <- tokens_wordstem(tokens_raw, language = "de")

stem_map <- tibble(
  stem = unlist(tokens_stem),
  form = unlist(tokens_raw)
) %>%
  count(stem, form, sort = TRUE)

suggested_terms_surface <- tibble(stem = suggested_terms_clean) %>%
  left_join(
    stem_map %>%
      group_by(stem) %>%
      slice_max(n, n = 3, with_ties = FALSE) %>%
      summarise(forms = paste(form, collapse = ", "), .groups = "drop"),
    by = "stem"
  )

available_dicts <- hunspell::list_dictionaries()
dict_id <- if ("de_DE_frami" %in% available_dicts) {
  "de_DE_frami"
} else if ("de_DE" %in% available_dicts) {
  "de_DE"
} else {
  NA_character_
}

suggested_terms_surface <- suggested_terms_surface %>%
  mutate(
    primary_form = stringr::word(forms, 1, sep = ", "),
    primary_form = stringr::str_replace_all(primary_form, "_", " "),
    dict_ok = if (is.na(dict_id)) TRUE else hunspell::hunspell_check(primary_form, dict = dict_id)
  )

# Co-occurrence filter: candidate should appear with relation terms
relation_terms_model <- normalize_terms_for_model(relation_terms)
dfm_model <- feminizid_dfm_trimmed
relation_in_docs <- rowSums(as.matrix(
  dfm_select(dfm_model, pattern = relation_terms_model)
)) > 0

cand_dfm <- dfm_select(dfm_model, pattern = suggested_terms_clean)
cand_mat <- as.matrix(cand_dfm) > 0
cand_docs <- colSums(cand_mat)
cooccur_docs <- colSums(cand_mat[relation_in_docs, , drop = FALSE])
cooccur_ratio <- cooccur_docs / pmax(cand_docs, 1)

candidate_scores <- tibble(
  stem = colnames(cand_mat),
  cand_docs = cand_docs,
  cooccur_docs = cooccur_docs,
  cooccur_ratio = cooccur_ratio
)

suggested_terms_surface <- suggested_terms_surface %>%
  left_join(candidate_scores, by = "stem")

# Heuristic cleanup for glued function words / artifacts
JUNK_SUFFIXES <- c("zwar", "pro", "und", "oder", "aber", "doch", "weil", "dass", "nicht")
junk_regex <- paste0("(", paste(JUNK_SUFFIXES, collapse = "|"), ")$")

suggested_terms_surface <- suggested_terms_surface %>%
  mutate(junk_suffix = stringr::str_detect(primary_form, junk_regex))

suggested_terms_final <- suggested_terms_surface %>%
  filter(
    dict_ok,
    nchar(primary_form) >= 4,
    !junk_suffix,
    cooccur_docs >= 5,
    cooccur_ratio >= 0.2
  ) %>%
  distinct(primary_form) %>%
  arrange(primary_form) %>%
  pull(primary_form)

suggested_terms_surface %>%
  arrange(desc(primary_form)) %>%
  head(30)

suggested_terms_final
```

## Second-pass API query (manual curation)

We run a stricter AND query to keep results manageable. The original keywords
are split into two groups (action vs. relation) and combined with curated
extra terms. Wildcards are included for single-word terms.

```{r second-pass-query}
# Split original keywords into action vs. relation/context (no new terms)
action_terms <- c(
  "Feminizid", "Femizid", "Frauenmord",
  "Tötung von Frau", "Mord an Frau",
  "Gewalt gegen Frauen", "Partnergewalt", "Beziehungsgewalt",
  "häusliche Gewalt", "Partnerschaftsgewalt"
)

relation_terms <- c(
  "Familiendrama", "Familientragödie", "Beziehungstat", "Ehestreit"
)

# Wildcards only for single-word terms from the original list
add_wildcards_single <- function(terms) {
  single <- terms[!grepl("\\s", terms)]
  paste0(single, "*")
}

action_terms <- unique(c(action_terms, add_wildcards_single(action_terms)))
relation_terms <- unique(c(relation_terms, add_wildcards_single(relation_terms)))

# Manual split from suggested_terms_final
SECOND_PASS_EXCLUDE <- c("gewalt")

action_terms_extra <- c(
  "ermordet", "erschossen", "erstochen", "getötet",
  "tötung", "tötungsversuch", "frauentötungen",
  "stranguliert", "kopfschuss", "küchenmesser",
  "mordopfer", "umgebracht", "tötete", "totschlag",
  "misshandlung", "misshandelt", "stalking",
  "eifersucht", "eifersüchtig", "eifersüchtige", "eifersüchtigen",
  "mord"

)

relation_terms_extra <- c(
  "ehepartner", "exmann", "intimpartner", "lebensgefährt",
  "lebensgemeinschaft", "Partner", "schwägerin", "Peiniger", "Frau",
  "ehefrau", "freundin", "beziehung", "geschieden", "mutter", "vater",
  "sohn", "tochter", "schwiegersohn", "ex-partner", "ex-mann", "ex-frau"

)

# Dropped terms (noisy/too generic): aufwies, desinteresseerklärung,
# desinteressenerklärung, erleben, erwachsenenleben, fünfte, häusliche,
# körper, männer, menos, mutmassliche, physische, rechercheplattformen,
# rechercheprojekt, sexuelle, straftatbestand, tatverdächtigen, türkin

action_terms_extra <- setdiff(action_terms_extra, SECOND_PASS_EXCLUDE)
relation_terms_extra <- setdiff(relation_terms_extra, SECOND_PASS_EXCLUDE)

action_terms_second <- unique(c(action_terms, action_terms_extra))
relation_terms_second <- unique(c(relation_terms, relation_terms_extra))

action_terms_second <- unique(c(action_terms_second, add_wildcards_single(action_terms_second)))
relation_terms_second <- unique(c(relation_terms_second, add_wildcards_single(relation_terms_second)))

second_pass_result <- execute_swissdox_workflow(
  client = client,
  query = create_swissdox_query(
    content_groups = list(
      list(operator = "OR", terms = action_terms_second),
      list(operator = "OR", terms = relation_terms_second)
    ),
    date_from = "1990-01-01",
    date_to = format(Sys.Date(), "%Y-%m-%d"),
    languages = "de",
    max_results = 1000000L,
    columns = c("id", "pubtime", "medium_code", "medium_name", "rubric",
                "regional", "doctype", "doctype_description", "language", 
                "char_count", "dateline", "head", "subhead", 
                "article_link", "content_id", "content")
  ),
  name = paste("Feminizid_SecondPass", format(Sys.time(), "%Y%m%d_%H%M%S")),
  comment = "Second-pass query with curated action+relation terms (1990+)",
  return_data = TRUE,
  output_dir = "data"
)
```

```{r second-pass-load-manual}
# Manual fallback: load a downloaded TSV if the API call timed out
manual_file <- "data/87abc702-4e8e-419c-8269-a81519a7ccff__2026_01_25T16_20_56.tsv.xz"
media_data_second <- read_tsv(
  manual_file,
  locale = locale(encoding = "UTF-8"),
  quote = "",
  col_types = cols(.default = "c")
)
save_checkpoint(media_data_second, "feminizid_raw_data_secondpass")
```

## Second-pass data preparation

Clean the second-pass data and build a corpus for downstream analysis.

```{r second-pass-prep}
load_checkpoint("feminizid_raw_data_secondpass", "media_data_second")

media_data_second$pubtime <- as.POSIXct(as.character(media_data_second$pubtime), tz = "UTC")
media_data_second$pub_date <- as.Date(media_data_second$pubtime)
media_data_second$year <- as.numeric(format(media_data_second$pubtime, "%Y"))

media_data_second <- media_data_second %>%
  filter(language == "de") %>%
  filter(!is.na(content), content != "", nchar(content) > 50) %>%
  filter(year >= 1990) %>%
  distinct(id, .keep_all = TRUE) %>%
  mutate(unique_id = paste0("doc_", row_number()))

media_data_second$clean_content <- media_data_second$content %>%
  str_remove_all("<[^>]+>") %>%
  str_remove_all("\\n|\\t|\\r") %>%
  str_remove_all("https?://\\S+") %>%
  str_squish()

secondpass_corpus <- corpus(
  media_data_second,
  text_field = "clean_content",
  docid_field = "unique_id"
)

save_checkpoint(secondpass_corpus, "feminizid_secondpass_corpus")
```

```{r second-pass-article-counts}
# Sanity check: article counts per year (1900-present)
articles_second_by_year <- media_data_second %>%
  mutate(
    pubtime = as.POSIXct(as.character(pubtime), tz = "UTC"),
    year = as.numeric(format(pubtime, "%Y"))
  ) %>%
  filter(!is.na(year), year >= 1990, year <= as.numeric(format(Sys.Date(), "%Y"))) %>%
  count(year)

ggplot(articles_second_by_year, aes(x = year, y = n)) +
  geom_col(fill = "steelblue", alpha = 0.8) +
  theme_minimal() +
  labs(title = "Second-pass articles per year",
       x = "Year", y = "Article count")
```

## Language change over time

Two plots: (1) action vs. relation term usage over time, (2) key terms that
distinguish early vs. late periods (1990-2004 vs. 2005+).

```{r second-pass-language-change}
wildcard_to_regex <- function(terms) {
  escaped <- stringr::str_replace_all(terms, "([\\.^$|()\\[\\]{}+?\\\\])", "\\\\\\1")
  stringr::str_replace_all(escaped, "\\*", ".*")
}

action_regex <- wildcard_to_regex(action_terms_v2)
relation_regex <- wildcard_to_regex(relation_terms_v2)

media_data_second <- media_data_second %>%
  mutate(
    has_action = str_detect(clean_content, regex(paste(action_regex, collapse = "|"), ignore_case = TRUE)),
    has_relation = str_detect(clean_content, regex(paste(relation_regex, collapse = "|"), ignore_case = TRUE))
  )

usage_by_year <- media_data_second %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarise(
    total_articles = n(),
    action_articles = sum(has_action, na.rm = TRUE),
    relation_articles = sum(has_relation, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    action_per_1000 = (action_articles / total_articles) * 1000,
    relation_per_1000 = (relation_articles / total_articles) * 1000
  )

usage_long <- usage_by_year %>%
  select(year, action_per_1000, relation_per_1000) %>%
  pivot_longer(-year, names_to = "category", values_to = "per_1000") %>%
  mutate(category = recode(category,
                           action_per_1000 = "Action terms",
                           relation_per_1000 = "Relation terms"))

p_lang1 <- ggplot(usage_long, aes(x = year, y = per_1000, color = category)) +
  geom_line(size = 1, alpha = 0.8) +
  theme_minimal() +
  labs(title = "Action vs. relation terms over time (per 1000 articles)",
       x = "Year", y = "Mentions per 1000 articles", color = "Category") +
  theme(legend.position = "bottom")

# Normalized contrast: early vs. late periods (per million words)
tokens_raw <- tokens(secondpass_corpus,
                     remove_punct = TRUE,
                     remove_symbols = TRUE,
                     remove_numbers = TRUE,
                     remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(all_stopwords, min_nchar = 3)

dfm_raw <- dfm(tokens_raw)
docvars(dfm_raw, "period") <- ifelse(docvars(secondpass_corpus, "year") < 2005,
                                     "1990-2004", "2005-2026")

dfm_period <- dfm_group(dfm_raw, groups = "period")
dfm_prop <- dfm_weight(dfm_period, scheme = "prop")

term_prop <- convert(dfm_prop, to = "data.frame") %>%
  select(-doc_id) %>%
  mutate(period = rownames(dfm_prop))

term_long <- term_prop %>%
  pivot_longer(-period, names_to = "term", values_to = "prop")

term_wide <- term_long %>%
  pivot_wider(names_from = period, values_from = prop, values_fill = 0) %>%
  mutate(
    early_per_million = `1990-2004` * 1e6,
    late_per_million = `2005-2026` * 1e6,
    log2_ratio = log2((late_per_million + 1) / (early_per_million + 1))
  )

top_contrast <- bind_rows(
  head(arrange(term_wide, desc(log2_ratio)), 15),
  head(arrange(term_wide, log2_ratio), 15)
) %>%
  mutate(direction = ifelse(log2_ratio > 0, "Later period", "Earlier period"))

p_lang2 <- ggplot(top_contrast, aes(x = reorder(term, log2_ratio), y = log2_ratio, fill = direction)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(title = "Key terms by period (log2 ratio, per million words)",
       x = "Term", y = "Log2 ratio", fill = "Direction")

print(p_lang1)
print(p_lang2)
```

